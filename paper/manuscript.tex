%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass[sigconf]{acmart}
\let\Bbbk\relax %% fix bug
\usepackage[utf8]{inputenc}

% =======================
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsopn}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{textcomp}

\usepackage{boxedminipage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{url}
\usepackage{times}
\usepackage{version}
% \usepackage[pdftex]{graphicx}
\usepackage{epsfig}
\usepackage{epsf}
%\usepackage{graphics}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\PassOptionsToPackage{bookmarks={false}}{hyperref}
%%%%%%%%%%%%
\usepackage{comment}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{dblfloatfix}
% ==========================
\input lstset.tex
\input macros.tex

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\algrenewcommand\alglinenumber[1]{\footnotesize #1}

\algnewcommand\algorithmicassume{\textbf{Assumption:}}
\algnewcommand\Assume{\item[\algorithmicassume]}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}

\algnewcommand\algorithmicparameter{\textbf{Parameter:}}
\algnewcommand\Parameter{\item[\algorithmicparameter]}


\begin{document}

\title{Rank-based Training-Free NAS Algorithm}

\author{Cheng-Han Hsieh}
\email{emiliaistruelove@gmail.com}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\author{Chun-Wei Tsai}
\email{cwtsai@mail.cse.nsysu.edu.tw}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\begin{abstract}
    The training-free neural architecture search (NAS) typically uses one  
    score function to evaluate how a neural network architecture performs. 
    However, every architecture has unique characteristics. To accurately 
    evaluate an architecture, the score function should take the different 
    aspects of an neural architecture into account. 
    To achieve the goal, we propose a rank-based training-free NAS algorithm, 
    which combines three ``complementary'' training-free score functions to 
	evaluate an architecture. 
	NAS without training score function measures the difference 
    of the activation patterns between a batch of data passing through an 
    architecture, i.e., the image distinction ability. 
	Noise immunity measures the difference between the pooling layers 
    of an architecture by passing an image and the same image with Gaussian noise, 
    i.e., the image generalization ability. 
	And LogSynFlow measures the importance of connections of an architecture, 
    i.e., the trainability. 
    Through the three measurements, an architecture can be evaluated from 
    different aspects. With an integrated score function, the proposed 
    method, \palg{} applies simulated annealing algorithm to obtain 
    better performance on searching high-accuracy architectures. 
    To evaluate the performance of \palg{}, this paper compared 
    it with several NAS algorithms, including weight sharing methods, 
    non-weight sharing methods, and several state-of-the-art training-free 
    score functions. The ultimate results show that \palg{} outperforms most 
    training-free score functions and shows the robustness between different 
    search spaces. 

\end{abstract}
\maketitle

\section{Introduction}
\label{sec:introduction}

% introduction
    Neural architecture search (NAS) has recently drawn a big amount of 
    attention, since the ability to automatically design a ``good'' neural 
    architecture. By leveraging machine learning algorithms 
    \cite{https://doi.org/10.48550/arxiv.1611.01578}, NAS algorithms can 
    automatically search numerous architectures to find out the ones 
    that outperform those designed by human experts. Recently, the use of NAS has been widespread, 
    from object detection \cite{https://doi.org/10.48550/arxiv.2111.13336}, 
    image recognition \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    and speech recognition \cite{https://doi.org/10.48550/arxiv.2011.05649} 
    to natural language processing \cite{jiang-etal-2019-improved}. 
    Despite the promising results of NAS, there are still many challenges 
    to deal with. One major problem is the extremely high computational 
    cost for searching an optimal architecture, which makes NAS 
    impractical for real-world applications, particularly on resource-constrained 
    platforms like embedded system. NAS is costly because 
    during searching, a candidate architecture must be trained to 
    evaluate how good of this architecture. %% compare architectures each of which must be trained

    To eliminate the training cost, recent works proposed lots of methods 
    which is so called training-free NAS. 
    % NASWOT
    For example, Mellor et al. \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    proposed the measurement of the correlation between the binary 
    activation paterns, induced by the untrained network at two inputs, 
    abbreviated as NASWOT. 
    % synflow
	Another example can be found in \cite{tanaka2020pruning}, Tanaka et al. 
    presented pruning a network by using Iterative Synaptic Flow Pruning 
    (SynFlow) algorithm which intends to deal with the layer collapse 
    problem. The score function used in the algorithm is so-called 
    \it{synaptic saliency} \rm{score}. Later, Abdelfattah et al. 
    \cite{abdelfattah2021zerocost} extended SynFlow to score a network 
    architecture by summing synaptic saliency score over all parameters 
    in the model. The outcome score represents the trainability of an 
    architecture. Cavagnero et al. \cite{Cavagnero_2023} pointed out that 
    SynFlow is likely to suffer from gradient explosion, then proposed 
    the LogSynFlow score function which prevents the problem. 
    % NTK
    Instead of using saliency metric to measure the trainability, Chen et al. 
    \cite{https://doi.org/10.48550/arxiv.2102.11535} proposed to compute 
    the condition number of neural tangent kernel (NTK) 
    \cite{https://doi.org/10.48550/arxiv.2109.00817}, which is used as 
    a score to estimate the trainability of an architecture. 

% what is unknown?
    Although the training-free score functions are capable of reducing the 
    computational cost, \xtab{table:corr_score} shows most score functions suffer %
    from the low correlation between score values and the final accuracy of 
    architectures. The shortcoming leads to a predicament that no matter 
    how good the searching algorithm is, the optimal architectures are still 
    buried in dirt. %
    The major problem that causes the low correlation is that a single score 
    function can only evaluate one characteristic of an architecture. 
    \begin{table}[tb]
        \caption{\textsc{The Kendall correlation between training-free score and test accuracy, evaluated on the three datasets of NATS-Bench-TSS \cite{Dong_2021}. }}
        \resizebox{0.45\textwidth}{!}{
            \begin{tabular}{cccc}\toprule
                Training-free score function & CIFAR-10 & CIFAR-100 & ImageNet-16-120 \\ 
                \midrule
                \bf{condition number of NTK}                     & -0.50    & -0.47     & -0.49 \\
                \bf{Snip}                    & 0.46     & 0.46      & 0.42  \\
                \bf{Grasp}                   & 0.38     & 0.37      & 0.39  \\
                \bf{NASWOT}                  & 0.62     & 0.65      & 0.62  \\
                \bf{SynFlow}                 & 0.53     & 0.51      & 0.52  \\
                \bf{LogSynFlow}              & 0.57     & 0.55      & 0.55  \\
                \bf{NI}                      & 0.57     & 0.59      & 0.55  \\
                \bf{Rank (ours)}             & -0.69    & -0.70     & -0.67 \\ \bottomrule
                \label{table:corr_score}
            \vspace{-\baselineskip}
            \end{tabular}
        }
    \end{table}
    % TSS
    %             C10 v/t        C100 v/t     I16 v/t  
    % ni        : 0.59/0.57      0.59/0.59    0.55/0.55
    % naswot    : 0.60/0.62      0.64/0.65    0.61/0.62
    % logsynflow: 0.51/0.57      0.54/0.55    0.53/0.55
    % synflow   : 0.48/0.53      0.51/0.51    0.50/0.52
    % ntk       : -0.50/-0.50    -0.50/-0.47  -0.49/-0.49
    % grasp     : 0.33/0.38      0.38/0.37    0.37/0.39
    % snip      : 0.41/0.46      0.46/0.46    0.41/0.42
    % rank      : -0.66/-0.69    -0.70/-0.70  -0.66/-0.67
    %
    \begin{figure*}[b]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/naswot.pdf}}
        \caption{A simple example to illustrate the procedure of NASWOT.}
        \label{fig:naswot}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}
% method
    To address the problem, we propose cooperating three heterogeneous 
    score functions by ranking, which allows every score function 
    evaluates an architecture without losing fairness. 
    %The first score function is noise immunity (NI) \cite{10092788}, as 
    %a measurement of the ability of image generalization. 
    %The second one is NASWOT, used as a score to estimate the ability of distinction 
    %of architecture and also a complement of NI. 
    %The last one is LogSynFlow used as the measurement of trainability 
    %and also as a complement to NI and NASWOT. 
    And with these three score functions, the evaluation of an 
    architecture is not simply from a single aspect but three different 
    parts, which is like estimating the weight of an object by not only 
    the length but its height and width. 

    % conclusion
    % contribution
    The major contribution of this paper can be summarized as follow:
    (1) A novel concept of ``complementary'' is presented and used as 
    a metric to choose score functions. 
    (2) Cooperating the score functions by ranking algorithm, 
    which prevents one score overwhelming others numerically. 
    (3) Applying simulated annealing algorithm on the ranking algorithm 
    to imporve the searching ability. 
    (4) Improving simulated annealing algorithm by using a historical 
    best set. 
    %orginzation
    The remainder of this paper is organized as follows: 
    \xsec{sec:related_work} provides the detail about the training-free 
    score functions. 
    \xsec{sec:proposed} gives a detailed description about the proposed method. 
    \xsec{sec:results} begins with parameters setting and provides the 
    simulation results in different search spaces. 
    The conclusion and further prospect are gived in \xsec{sec:conclusion}.

    \section{Related Works}
    \label{sec:related_work}
    \subsection{Problem Definition}
    In \cite{10092788}, the NAS is defined as following: 
    first, assume that $D_{\text{train}}$ and $D_{\text{test}}$ are two subsets of input data $D$. 
    Assume $\mathcal F_A$ is the accuracy function, $\mathcal F_S$ the score function, $\mathcal A_s$ the 
    search algorithm of NAS, and $\mathcal A_T$ the training function. Then, the NAS problem can be 
    described as an optimization problem as follows: 
    \begin{equation}
        \label{equ:nas}
        \mathbb N^*=\max_{\mathbb N^b\in\mathbb N}\mathcal F_A(\mathcal A_T(\mathbb N^b, D_{\text{train}}), D_{\text{test}}),
    \end{equation}
    where $\mathbb N$ is a set of neural architectures, namely, the search space of NAS. 
    For non-training-free NAS 
    \begin{equation}
        \label{equ:non-training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_A(\mathcal A_T(\mathbb N^s, D_{\text{train}}), D_{\text{test}}), \mathbb N),
    \end{equation}
    and for training-free NAS 
    \begin{equation}
        \label{equ:training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_S(\mathbb N^s, D_{\text{train}}), \mathbb N),
    \end{equation}
    Since the training function is replaced by training-free NAS algorithms, they need much less 
    computational resource than non-training-free NAS algorithms. 
    Training-free NAS is comprised of two part, training-free score function $\mathcal F_S$ and 
    search algorithm $\mathcal A_S$. 

    \subsection{Training-free Score Functions}
    In \cite{https://doi.org/10.48550/arxiv.2006.04647}, Mellor et al. 
    proposed a score function without the requirement for training, NASWOT. 
    \xfig{fig:naswot} gives a simple example 
    to illustrate the procedure of NASWOT score function. Consider a 
    mini-batch of data $X=\{x_i\}^N_{i=1}$ passing through a neural network 
    architecture. The $j$-th activated ReLU units in every layer of the architecture 
    form a binary code $c_{i,j}$ that define the linear region. The correlation 
    between binary codes for the whole mini-batch can be examined by computing 
    the kernel matrix 
    \begin{equation}
        K_H=\begin{pmatrix}L-\sum_{i=1}^{N_A}d_H(c_{1,i},c_{1,i})&\cdots&L-\sum_{i=1}^{N_A}d_H(c_{1,i},c_{N,i})\\\vdots&\ddots&\vdots\\L-\sum_{i=1}^{N_A}d_H(c_{N,i},c_{1,i})&\cdots&L-\sum_{i=1}^{N_A}d_H(c_{N,i},c_{N,i})\end{pmatrix},
    \end{equation}
    where $N_A$ is the number of ReLU units, $d_H(\cdot,\cdot)$ is the hamming 
    distance between two binary codes, and $L$ is the total length of the binary 
    code. With the kernel matrix, the score of an architecture is evaluated as 
    follow: 
    \begin{equation}
        s=\log\lvert K_H\rvert,
    \end{equation}
    The rationale behind is that, the more different between the binary codes 
    the better the architecture learns. And the determinant of the kernel matrix 
    measures how ``different'' they are by calculating the volume formed by the 
    row vector of $K_H$. 

    \begin{figure*}[tb]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/ni.pdf}}
        \caption{A simple example to illustrate the procedure of NI.}
        \label{fig:ni}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}

    Wu et al. \cite{10092788} discovered in some cases, an architecture with a high %%
    NASWOT score may classify the same kind of input data into different classes. 
    \xfig{fig:ni} gives an example to illustrate how NI evaluates an architecture. 
    The score function picks a mini-batch of data, denoted $X$, and then applies 
    Gaussian noise to it. The process is defined by $X'=X+z$ where $z$ is the 
    Gaussian noise. By passing $X$ and $X'$ through the untrained architecture, 
    then computing the difference of square of each feature map captured at all 
    pooling layers. Calculate the matrix $\kappa$ defined by 
    \begin{equation}
        \label{equ:ni_kappa}
        \kappa=\begin{pmatrix}\frac{(\tau_{1,1}-\tau'_{1,1})^2}{\lvert \tau_{1,1}\rvert}&\cdots&\frac{(\tau_{N,1}-\tau'_{N,1})^2}{\lvert \tau_{N,1}\rvert}\\\vdots&\ddots&\vdots\\\frac{(\tau_{1,N_P}-\tau'_{1,N_P})^2}{\lvert \tau_{1,N_P}\rvert}&\cdots&\frac{(\tau_{N,N_P}-\tau'_{N,N_P})^2}{\lvert \tau_{N,N_P}\rvert}\end{pmatrix},
    \end{equation}
    where $N_P$ is the total number of pooling layers; $N$ the size of mini-batch, 
    and $\tau_{i,j}$ and $\tau'_{i,j}$ are the feature maps which $X$ passing and 
    the one perturbed by $X'$ at the $j$-th pooling layer and $i$-th input 
    data of the mini-batch, respectively. 
    Then $\eta$ can be calculated by 
    \begin{equation}
        \label{equ:ni_eta}
        \eta=\ln(\epsilon+e_L\kappa e_N),
    \end{equation}
    where $\epsilon$ is a small positive number, and $e_{N_P}$ and $e_N$ are a row 
    vector of 1's of size $N_P$ and a column vector of 1's of size $N$, respectively. 
    According to the fact that the input data are the same kind, the smaller $\eta$ 
    is, the better the noise immunity of the architecture is. 

    % synflow
    Besides image-related measurements, there is another characteristic to 
    evaluate a network, trainability. 
    %%\textit{The Lottery Ticket Hypothesis} \cite{frankle2019lottery} reveals that a 
    %%network may have a 'core' which decides the final accuracy of the network. %%
    %%How to prune a network correctly is thereby tempting. Lee et al. 
    \cite{lee2019snip} presented an algorithm that uses synaptic saliency score as a 
    criterion to prune the network, named single-shot network pruning (SNIP). 
    The synaptic saliency score is defined by 
    \begin{equation}
        \label{equ:snip_connection_sensitivity}
        s_j=\lvert \frac{\partial \mathcal L}{\partial w_j}w_j\rvert,
    \end{equation}
    where $w_j$ the $j$-th element of the weight of the netwrok, and $\mathcal L$ is 
    the empirical risk. The meaning behind is to approximate the contribution 
    to the change of the loss function from a specific connection. By pruning the 
    connections that are relatively not contributive, the expensive prune, retrain 
    cycles, can be prevented. 
    Later, Wang et al. \cite{wang2020picking} noticed that SNIP with a high pruning ratio 
    tends to cause \textit{layer-collapse}, which prunes all parameters in a single weight 
    layer. Therefore, they propose Gradient Signal Preservation (GraSP) algorithm, which 
    aims to preserve the gradient flow at initialization by approximating the change in 
    gradient norm defined as 
    \begin{equation}
        \label{equ:grap}
        S_p(\theta)=-(\textbf{H}\frac{\partial \mathcal L}{\partial \theta})\odot\theta,
    \end{equation}
    where $\textbf{H}$ is the Hessian matrix, $\theta$ the parameters, and $\odot$ is Hadamard product. 
    In \cite{tanaka2020pruning}, to solve the problem that the existing pruning algorithms using 
    global masking, e.g., Magnitude, SNIP, GraSP, usually encounter layer-collapse, which makes 
    the pruned network untrainable. 
    Tanaka et al. generalized the synaptic saliency scores as 
    \begin{equation}
        \label{equ:synflow}
        S_p(\theta)=\frac{\partial \mathcal R}{\partial \theta}\odot\theta,
    \end{equation}
    where $\mathcal R$ is a scalar loss function, and proposed Iterative Synaptic Flow Pruning (SynFlow) 
    algorithm, an extension of magnitude pruning algorithm that avoids layer-collapse. 
    Abdelfattah et al. \cite{abdelfattah2021zerocost} extended the work, and presented to score a 
    network by summing the synaptic saliency score over all parameters in the model, which is 
    defined as 
    \begin{equation}
        \label{equ:zero_cost}
        S_n=\sum^N_i S_p(\theta)_i.
    \end{equation}
    The rationale behind is to calculate all the contributions to the loss function of parameters. 
    The higher the score of an architecture, the more trainable this architecture is. 
    Finally, Cavagnero et al. \cite{Cavagnero_2023} improved SynFlow, which is likely to fall into 
    gradient explosion problem, and proposed LogSynFlow which scales down the gradient. 
    The formula is defined by 
    \begin{equation}
        \label{equ:logsynflow}
        S(\theta)=\theta\cdot\log(\frac{\partial \mathcal L}{\partial \theta}+1).
    \end{equation}

    Except using the synaptic saliency score to measure the trainability, 
    \cite{https://doi.org/10.48550/arxiv.2102.11535} uses the condition number of neural tangent 
    kernel (NTK) \cite{jacot2020neural} to measure it, which the score can be defined as 
    \begin{equation}
        \label{equ:cn_ntk}
        \frac{\lambda_m}{\lambda_0}, 
    \end{equation}
    where $\lambda_0$ is the biggest eigenvalue and $\lambda_m$ is the smallest eigenvalue of the 
    NTK. The rationale behind is the converge rate of the network is condition by the rate 
    $\frac{\lambda_m}{\lambda_0}$. The higher the condition is, the faster the converge. 

    \subsection{Search Algorithms}
    %%
    \begin{figure*}[htb]
        \vspace{-\baselineskip}
        \begin{center}
            \begin{tabular}{ccc}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/naswot-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ni-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ninaswot-acc.pdf}}}
            \end{tabular}
            \caption{(a) NASWOT score for 1,000 randomly chosen architectures from NAS-Bench-201 in the CIFAR-10 dataset 
            (b) NI score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset. 
            (c) NI score + NASWOT score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset.}
            \label{fig:ninaswot}
        \end{center}
        \vspace{-\baselineskip}
    \end{figure*}
    %%
    There are several search algorithms applied on NAS, including random search, reinforcement learning, and
    metaheuristic algorithms. 
    In \cite{https://doi.org/10.48550/arxiv.2006.04647} \cite{Lopes_2021}, random search is applied. 
    The advantage of random search is simple and easy to implement, and the result can be taken as a 
    baseline compared to more comprehensive search algorithms. Generally speaking, when applying random 
    search on a small search space, e.g., NAS-Bench-201 \cite{dong2020nasbench201}, the performance is 
    similar to other search algorithms. But when it comes to a relatively larger search space, e.g., NAS-Bench-101 
    \cite{ying2019nasbench101} and NATS-Bench-SSS \cite{Dong_2021}, random search can no longer standout 
    in other search algorithms. 

    In \cite{zoph2017neural}, reinforcement learning is used to find the maximum accuracy of an architecture 
    generated by a network architecture controller. The actions $a_{1:T}$ of the reinforcement learning is 
    equivalent to updating the parameters $\theta_c$ for the controller. The goal is to maximize 
    its expected reward, defined by 
    \begin{equation}
        \label{equ:reinforcement_rw}
        J(\theta_c)=E_{P(a_{1:T};\theta_c)}[R], 
    \end{equation}
    and the gradient of $J(\theta_c)$ is defined by
    \begin{equation}
        \label{equ:reinforcement_grad}
        \nabla_{\theta_c} J(\theta_c)=\sum^T_t=1 E_{P(a_{1:T};\theta_c)}[\nabla_{\theta_c}\log P(a_t\mid a_{(t-1):1};\theta_c)R]. 
    \end{equation}

    As for an example of metaheuristic algorithm, in \cite{10.1145/3491396.3506510}, Wu et al. leveraged 
    genetic algorithm (GA) as a search strategy. 
    By encoding the network architecture, the architecture can be viewed as a gene. Therefore, GA can be 
    easily applied. 
    Later in \cite{10092788}, Wu et al. used search economic (SE) \cite{7379579} as a search strategy. The 
    basic idea of SEs is to first divide the search space into a set of subspaces and investigate those 
    subspaces based on the expected value of each subspace. The expected value is comprised of 
    (1) The number of times the subspace has been investiagted. 
    (2) The average objective value of the new candidate solutions in the subspace at current iteration. 
    (3) The best solution so far in this subspace. 
    Based on these designs, SE avoids falling into local optimum, while searching for high expected value instead. 
    
    \section{Methods}
    \label{sec:proposed}
    
    The motivation origins from these three questions: 

    \textit{1. Can we combine multiple score functions to improve the search performance 
    or the correlation between score values and the final accuracy of architectures?}
    
    The answer is yes. If the chosen score functions are complementary, the new correlation 
    can overtake the original ones. \cite{10.1145/3491396.3506510} 
    shows that possibility. When an architecture gains a high NASWOT score, 
    it may misclassify images in the same class into different classes. NI here comes 
    to the rescue. By measuring the noise immunity of an architecture, the misclassifying problem 
    can be solved. Thus, combine NASWOT and NI, the correlation and performance increase. 
    An example shows in \xfig{fig:ninaswot}.

    \textit{2. How to choose the score functions?}

    As mentioned, the key is to choose the complement of a score function. For example, 
    score functions that evaluate the expressivity/distinction ability should have other 
    score functions to evaluate the generalization ability. For these two graphic recognition 
    score functions, the complements can be measurements of trainability. 

    \textit{3. How to combine multiple kinds of score functions together 
    and prevent one from overwhelming the others numerically?}
    
    One possible way is normalization, but the necessary information is known only 
    after evaluating the whole search space, e.g., range, mean, or standard deviation, 
    which is practically impossible. To make the presented score function 
    agnostic to the search space, it leverages multiple score functions by ranking, 
    which also provides a solution to have equal contributions from each score function. 

    \subsection{Simulated Annealing with Ranking Algorithm}
    
    The simulated annealing (SA) algorithm with ranking algorithm (\palg{})
    is outlined in \xalg{alg:SA}. In line 1-2, the historical best architectures 
    set, denoted $H$, is initialized to an empty set, and an architecture is randomly sampled from 
    search space, denoted $s$. Line 3-18 are the major part of \palg{}, when the temperature 
    is lower than ending temperature, the algorithm is terminated. Line 4-16 
    represent every time the temperature decreases, $t$ iterations with following steps are executed. 
    In line 5, $s$ is used as a seed to generate the neighborhood of it, denoted $U$, 
    and the size of it is decided by a presetting parameter, the number of neighbours, $N$. 
    In line 6, $s$, $U$ and the architectures recorded in historical best architectures set are 
    put into ranking function. 
    After ranking, the best architecture (including $s$) is added into $H$, and the size of the set is 
    maintained by removing the low-rank architectures. 
    And the best architecture (excluding $s$) which has the highest rank is denoted by $s'$. 
    In line 9, the difference of the rank between $s'$ and $s$ is calculated and detnote $\Delta E$. 
    Then in line 10, a random variable $x$ is sampled from the uniform distribution on $[0,1]$. 
    Line 11-15 are the transition process. If the rank of $s'$ is higher than $s$ ($\Delta E<0$), 
    the current solution is replaced by $s'$, or be replaced by probability, defined by 
    \begin{equation} 
        \label{equ:SA_rk_prob}
        e^{-\alpha\frac{\Delta E}{\Psi}},
    \end{equation}
    where $\Psi$ is the current temperature, $\Delta E$ the difference of the rank between 
    $s$ and $s'$, and $\alpha$ is a constant number adjusts the probability. The more 
    the difference between $s$ and $s'$, the smaller the probability of accepting the new solution. 
    After the $t$ iterations, the temperature scales down by $r_\Psi$ in line $17$. When $\Psi$ is 
    lower or equal to $\tau$, the algorithm is terminated and the current architecture is returned in line $19$. 
    Notbly, the size of $H$ is based on the maximum search number of SA, defined by 
    \begin{equation}
        \label{equ:SA_rk_size_of_H}
        \beta (I\cdot N\cdot\frac{\ln{\tau/\Psi}}{\ln{r_\Psi}}),
    \end{equation}
    where $\beta$ is the coefficient of the size of $H$ and is a constant number between 0 and 1.

    \begin{algorithm}[h]
        \caption{The Simulated Annealing with Ranking Algorithm}\label{alg:SA}
        \begin{algorithmic}[1]
            \Input{search space $\mathbb N^*$}
            \Parameter Initial temperature $\Psi$, ending temperature $\tau$, a real number between 0 and 1, $r_\Psi$, the number of iteration, $t$, the number of generated neighbours, $N$, a real number, $\alpha$, the coefficient of the size of $H$, $\beta$
            \State Initialize historical best architectures set $H$ to empty set
            \State $s=\text{Randomly sample an architecture from }\mathbb N^*$
            \While{$\Psi>\tau$}
                \For{$\text{each }i \in [1\ldots t]$}
                    \State $U = \text{Neighbor}(s, N)$
                    \State $r=\text{Rank}(\{s\}\cup U\cup H)$
                    \State $\text{Maintain}(H)$
                    %\State $s'=\text{Best architecture excluding }s$
                    \State $s'=\arg\min_{\mathbb N^i\neq s}(r)$
                    \State $\Delta E=r(s')-r(s)$
                    \State $x\sim U[0,1]$
                    \If{$\Delta E<0$}
                        \State $s\leftarrow s'$
                    \ElsIf{$x\leq e^{-\alpha\frac{\Delta E}{\Psi}}$}
                            \State $s\leftarrow s'$
                    \EndIf
                \EndFor
                \State $\Psi\leftarrow \Psi\times r_\Psi$
            \EndWhile
            \State $\textbf{Return }s$ \Comment{Return the solution}
        \end{algorithmic}
    \end{algorithm}

    An example of an iteration in \palg{} is shown in \xfig{fig:SA-rank-iteration}. 
    Assume that at the beginning there is a current solution $s$ and the historical best 
    architectures set $H=\{H_1,H_2\}$. Next, the neighborhood of $s$ is generated, 
    denoted $U=\{s_A,s_B,s_C,s_D,s_E\}$. The rank of the architectures in the union 
    set of $H$, $\{s\}$ and $U$ are calculated. The architecture with highest rank 
    $s_C$ will be recorded by $H$, and the architecture with highest rank excluding 
    $s$ is denoted $s'$. Then the difference $\Delta E$ of the rank is used for 
    solution transition. If $\Delta E<0$, the solution is better than the the current 
    solution. If it is not better, the solution can still be accepted by probability 
    $e^{-\alpha\frac{\Delta E}{\Psi}}$. 
    
    \begin{figure}[htb]
        \begin{center}
            \resizebox*{0.48\textwidth}{!}{\includegraphics{asset/SA-rank-iteration.pdf}}
            \caption{An example of an iteration of \palg{}.}
            \label{fig:SA-rank-iteration}
        \end{center}
    \end{figure}

    \subsection{Encoding Schema}

    The encoding schema determines the efficiency of searching. The naïve encoding 
    schema is to use a binary or integer vector as a representation of an architecture. 
    For example, NAS-Bench-201 provides 5 operations and 6 nodes for a cell, 
    and cells form an architecture, which implies the whole search space can be 
    encoded into 6-integer vectors. However, a broken architecture appears since 
    one of the operations is ``zeroize'', which means the measurement of a broken 
    architecture is meaningless. To exclude the broken architectures, the encoding 
    schema in \cite{10092788} is used in this paper. The detail of the encoding 
    schema differs from search spaces, but they all share the common core idea, 
    preserving the critical path in architecture. The encoding schema guarantees 
    the existence of at least one path from input to output in a cell, and no 
    zeroize operation is in it. By excluding the broken architectures, the efficiency 
    improves. 

    \subsection{Ranking Algorithm}

    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NATS-Bench-SSS.}}
          \vspace{-\baselineskip}
        \begin{tabular}{@{}lccccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
                            &          & validation       & test             & validation       & test              & validation       & test \\ \midrule
                            &          &                  &                  & \textbf{Non-weight sharing} &        &                  &                  \\
        AREA                & $12,000$ & $84.62 \pm 0.41$ & $93.16 \pm 0.16$ & $59.24 \pm 1.11$ & $69.56 \pm 0.96$  & $37.58 \pm 1.09$ & $45.30 \pm 0.91$ \\
        REA                 & $12,000$ & $90.37 \pm 0.20$ & $93.22 \pm 0.16$ & $70.23 \pm 0.50$ & $70.11 \pm 0.61$  & $45.30 \pm 0.69$ & $45.94 \pm 0.92$ \\
        RS                  & $12,000$ & $90.10 \pm 0.26$ & $93.03 \pm 0.25$ & $69.57 \pm 0.57$ & $69.72 \pm 0.61$  & $45.01 \pm 0.74$ & $45.42 \pm 0.86$ \\
        RL                  & $12,000$ & $90.25 \pm 0.23$ & $93.16 \pm 0.21$ & $69.84 \pm 0.59$ & $69.96 \pm 0.57$  & $45.06 \pm 0.77$ & $45.71 \pm 0.93$ \\
        BOHB                & $12,000$ & $90.07 \pm 0.28$ & $93.01 \pm 0.24$ & $69.75 \pm 0.60$ & $69.90 \pm 0.60$  & $45.11 \pm 0.69$ & $45.56 \pm 0.81$ \\ \midrule
                            &          &                  &                  & \textbf{Training-free} &             &                  &                  \\
        RS-NI (N=1,000)         & $293$& $89.55 \pm 0.16$ & $92.56 \pm 0.21$ & $67.67 \pm 0.91$ & $67.61 \pm 1.06$  & $43.19 \pm 0.73$ & $43.22 \pm 0.53$ \\ 
        RS-NASWOT (N=1,000)     & $208$& $89.25 \pm 0.44$ & $92.21 \pm 0.31$ & $65.09 \pm 2.89$ & $64.61 \pm 3.16$  & $41.29 \pm 2.32$ & $41.35 \pm 2.31$ \\ 
        RS-LogSynFlow (N=1,000) & $123$& $89.60 \pm 0.11$ & $92.61 \pm 0.18$ & $68.23 \pm 0.49$ & $68.30 \pm 0.54$  & $43.58 \pm 0.58$ & $43.48 \pm 0.40$ \\ 
        RS-rank (ours) (N=1000) & $612$& $89.59 \pm 0.14$ & $92.51 \pm 0.17$ & $67.56 \pm 0.75$ & $67.55 \pm 0.82$  & $43.36 \pm 0.62$ & $43.27 \pm 0.45$ \\ 
        \palg{} (ours)          & $725$& $90.33 \pm 0.15$ & $93.26 \pm 0.24$ & $69.89 \pm 0.48$ & $70.24 \pm 0.42$  & $45.41 \pm 0.41$ & $46.43 \pm 0.75$ \\ \midrule
        %SA-NI                  &      & $90.24 \pm 0.10$ & $93.20 \pm 0.19$ & $69.83 \pm 0.56$ & $70.10 \pm 0.40$  & $45.49 \pm 0.50$ & $45.98 \pm 0.89$
        %SA-NASWOT              &      & $90.21 \pm 0.30$ & $93.16 \pm 0.28$ & $69.16 \pm 1.28$ & $69.27 \pm 1.46$  & $44.89 \pm 1.27$ & $45.49 \pm 1.33$
        Optimal             &          & $90.71$          & $93.65$          & $70.92$          & $71.34$           & $46.73$          & $47.40$          \\ \bottomrule
        \end{tabular}
        \label{table:overall_sss}
          \vspace{-\baselineskip}
    \end{table*}

    The general proposed ranking algorithm is in \xalg{alg:rank-based}. 
    Line 1-3 are to evaluate the network architectures in the subset 
    by each score function, and then the rank of each architecture is 
    obtained through the index of sorted scores. The subset, denoted 
    $\mathbb N^s$, is sampled from the search space, $\mathbb N^*$. And 
    the $j$-th score function is $\mathcal F_j(\cdot)$. The rank of the 
    $i$-th architecture in the results of $j$-th score function are 
    $r_{\mathcal F_j}(\mathbb N^i)$, as shown in line 2. In Line 4, 
    the final rank of $i$-th architecture is calculated by summing up 
    its rank in $j$-th score function. Then the ranking result of each 
    architecture is returned. In summary, the ranking algorithm makes the 
    contribution of each score function even and evaluate an architecture 
    from different aspects. If an architecture gains higher score in these 
    score functions, the final rank should be higher, i.e., smaller index. 
    Therefore, the highest rank (numerically lowest) in $r$ is the best 
    network architecture. In practical implementation, to reduce the 
    evaluation time of training-free score functions, the scores of 
    architecture can be stored and used in the future. 
    %The first step is to evaluate the network architectures in the subset, 
    %denoted $\mathbb N^s$, of search space, denoted $\mathbb N^*$. The 
    %evaluation of an architecture determined by $j$-th score function 
    %is $s_{\mathcal F_j}(\mathbb N^i)$. After evaluating the subset 
    %of search space, the rank of each score function is calculated, denoted 
    %$r_{\mathcal F}$. Then the final rank, $r$, calculated by summing up 
    %the ranks. If a network architecture gains higher score in these score 
    %functions, the final rank should be higher (smaller index) too. Therefore, 
    %the highest (minimum) rank in $r$ shall be the best network architecture. 
    %In summary, the ranking algorithm makes the contribution of 
    %each score function even and therefore evaluate an architecture from 
    %different aspects. 

    \begin{algorithm}[h]
        \caption{The Ranking Algorithm}\label{alg:rank-based}
        \begin{algorithmic}[1]
            \Input{A subspace, ${\mathbb N}^{s}$, from search space, ${\mathbb N}^{*}$}
            \For{$\text{each }j\in\{1,\ldots,M\}$}
                \State $r_{{\mathcal F}_{j}}({\mathbb N}^i)$ = index of ${\mathbb N}^i$ in list $[{\mathcal F}_{j}({\mathbb N}^1),\ldots,{\mathcal F}_{j}({\mathbb N}^{\lvert {\mathbb N}^s\rvert})]$ \par sorted by descending order
            \EndFor
            \State $r({\mathbb N}^i)=\sum^M_{j=0} r_{{\mathcal F}_{j}}({\mathbb N}^i)$
            \State $\textbf{Return }r$
        \end{algorithmic}
    \end{algorithm}

    The next question is choosing ``complementary'' score functions. The recent training-free 
    score functions can be roughly classified into different kinds. NASWOT, 
    linear regions distribution \cite{https://doi.org/10.48550/arxiv.2102.11535} \cite{lin2021zennas}, 
    maximum entropy of architecture \cite{sun2022maedet} are measurements 
    to the ability of distinction and expressivity of an architecture; 
    NI is a measurement to image generalization ability of an architecture; 
    the condition number of NTK, modified version of SNIP, GraSP, SynFlow, and 
    LogSynFlow are measurements to trainability of an architecture. 
    As mentioned, the chosen score function should cover every characteristics 
    of an architecture. In this paper, NI and NASWOT are a complementary set, 
    which takes the ability of image generalization and distinction into account. 
    This set is viewed as the measurement of the ability of graphic 
    recognition. Therefore, the last missing piece is the trainability of an 
    architecture, which, LogSynFlow is chosen to measure it. 

    \section{Experiment Results}
    \label{sec:results}

    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NATS-Bench-TSS.}}
        \vspace{-\baselineskip}
        \begin{tabular}{@{}lrcccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
                            &          & validation           & test             & validation           & test             & validation       & test \\ \midrule
                            &          &                      &                  & \textbf{Non-weight sharing} &           &                  &                  \\
        AREA                & $12,000$ & $91.18 \pm \z{0.43}$ & $93.95 \pm 0.39$ & $71.84 \pm 1.21$ & $71.92 \pm 1.29$ & $45.04 \pm 1.03$ & $45.40 \pm 1.14$ \\
        REA                 & $12,000$ & $91.25 \pm \z{0.31}$ & $94.02 \pm 0.31$ & $72.28 \pm 0.95$ & $72.23 \pm 0.84$ & $45.71 \pm 0.77$ & $45.77 \pm 0.80$ \\
        RS                  & $12,000$ & $91.07 \pm \z{0.26}$ & $93.86 \pm 0.23$ & $71.46 \pm 0.97$ & $71.55 \pm 0.97$ & $45.03 \pm 0.91$ & $45.28 \pm 0.97$ \\
        RL                  & $12,000$ & $91.12 \pm \z{0.25}$ & $93.90 \pm 0.26$ & $71.80 \pm 0.94$ & $71.86 \pm 0.89$ & $45.37 \pm 0.74$ & $45.64 \pm 0.78$ \\
        BOHB                & $12,000$ & $91.17 \pm \z{0.27}$ & $93.94 \pm 0.28$ & $72.04 \pm 0.93$ & $72.00 \pm 0.86$ & $45.55 \pm 0.79$ & $45.70 \pm 0.86$ \\ \midrule
                            &          &                      &                  & \textbf{Weight sharing} &               &                  &                  \\
        RSWS                & $4,154$  & $87.60 \pm \z{0.61}$ & $91.05 \pm 0.66$ & $68.27 \pm 0.72$ & $68.26 \pm 0.96$ & $39.73 \pm 0.34$ & $40.69 \pm 0.36$ \\
        DARTS-V1            & $5,475$  & $49.27 \pm 13.44$    & $59.84 \pm 7.84$ & $61.08 \pm 4.37$ & $61.26 \pm 4.43$ & $38.07 \pm 2.90$ & $37.88 \pm 2.91$ \\
        DARTS-V2            & $16,114$ & $58.78 \pm 13.44$    & $65.38 \pm 7.84$ & $59.48 \pm 5.13$ & $60.49 \pm 4.95$ & $37.56 \pm 7.10$ & $36.79 \pm 7.59$ \\
        GDAS                & $11,183$ & $89.68 \pm \z{0.72}$ & $93.23 \pm 0.13$ & $68.35 \pm 2.71$ & $68.17 \pm 2.50$ & $39.55 \pm 0.00$ & $39.40 \pm 0.00$ \\
        SETN                & $16,787$ & $90.00 \pm \z{0.97}$ & $92.72 \pm 4.52$ & $69.19 \pm 1.42$ & $69.36 \pm 1.72$ & $39.77 \pm 0.33$ & $39.51 \pm 0.33$ \\
        ENAS                & $7,061$  & $90.20 \pm \z{0.00}$ & $93.76 \pm 0.00$ & $70.21 \pm 0.71$ & $70.67 \pm 0.62$ & $40.78 \pm 0.00$ & $41.44 \pm 0.00$ \\ \midrule
                            &          &                      &                  & \textbf{Training-free} &                &                  &                  \\
        RS-NI (N=1,000)         & $592$  & $90.03 \pm \z{0.30}$ & $93.21 \pm 0.63$ & $70.27 \pm 0.70$ & $70.40 \pm 0.84$ & $42.92 \pm 2.18$ & $43.34 \pm 2.43$ \\ 
        RS-NASWOT (N=1,000)     & $201$  & $89.63 \pm \z{0.94}$ & $92.80 \pm 1.07$ & $69.76 \pm 1.25$ & $69.66 \pm 1.23$ & $43.51 \pm 2.90$ & $43.84 \pm 2.93$ \\ 
        RS-LogSynFlow (N=1,000) & $90$   & $89.67 \pm \z{0.89}$ & $92.92 \pm 0.70$ & $69.19 \pm 1.86$ & $68.98 \pm 1.97$ & $39.93 \pm 6.34$ & $40.23 \pm 6.45$ \\ 
        RS-rank (ours) (N=1000) & $887$  & $90.07 \pm \z{0.41}$ & $93.32 \pm 0.42$ & $70.06 \pm 1.34$ & $70.10 \pm 1.45$ & $43.68 \pm 2.11$ & $44.07 \pm 2.19$ \\ 
        TE-NAS                  & $459$  & $90.02 \pm \z{0.56}$ & $93.31 \pm 0.45$ & $70.12 \pm 1.04$ & $70.26 \pm 1.07$ & $43.15 \pm 2.23$ & $43.49 \pm 2.26$ \\
        \palg{} (ours)          & $902$  & $90.20 \pm \z{0.24}$ & $93.53 \pm 0.22$ & $70.51 \pm 0.71$ & $70.63 \pm 0.66$ & $43.80 \pm 1.86$ & $44.63 \pm 1.64$ \\ \midrule
        %SA-NI                  &        & $90.06 \pm \z{0.29}$ & $93.38 \pm 0.50$ & $70.41 \pm 0.74$ & $70.34 \pm 1.34$ & $42.69 \pm 1.87$ & $43.22 \pm 1.93$
        %SA-NASWOT              &        & $89.11 \pm \z{1.22}$ & $92.22 \pm 1.32$ & $69.38 \pm 1.23$ & $69.48 \pm 1.46$ & $41.77 \pm 3.96$ & 
        Optimal             &          & $91.61$              & $94.37$          & $73.49$          & $73.51$          & $46.73$          & $47.31$          \\ \bottomrule
        \end{tabular}
        \label{table:overall_201}
        \vspace{-\baselineskip}
    \end{table*}

    We evalute the performance of the proposed algorithm, \palg{}, 
    for searching the architectures of high accuracy. For comparison, several NAS 
    algorithms are included, e.g., random search (RS), regularized evolution algorithm 
    (REA) \cite{real2019regularized}, reinforcement learning (RL) \cite{Williams:92}, 
    Bayesian optimization with hyper band (BOHB) \cite{falkner2018bohb}, RS with 
    weight sharing (RSWS) \cite{li2019random}, differentiable architecture search 
    with first order (DARTS-V1) \cite{liu2019darts}, DARTS with second order 
    (DARTS-V2) \cite{liu2019darts}, gradient-based search using differentiable 
    architecture sampler (GDAS) \cite{dong2019searching}, self-evaluated template 
    network (SETN) \cite{Dong_2019}, efficient NAS (ENAS) \cite{pham2018efficient}, 
    assisted REA (AREA) \cite{https://doi.org/10.48550/arxiv.2006.04647}, 
    %zero-cost NAS \cite{https://doi.org/10.48550/arxiv.2101.08134}, 
    TE-NAS \cite{https://doi.org/10.48550/arxiv.2102.11535}, RS with NASWOT (RS-NASWOT) \cite{https://doi.org/10.48550/arxiv.2006.04647}, 
    RS with noise immunity (RS-NI) \cite{10092788}, RS with LogSynFlow (RS-LogSynFlow) \cite{Cavagnero_2023}, 
    and RS with ranking (RS-rank). 

    The three search spaces, NATS-bench-SSS, NATS-bench-TSS 
    \cite{Dong_2021} and NAS-Bench-101 \cite{ying2019nasbench101}, are used 
    here to evalute the performance of the NAS algorithms. The architectures 
    of NATS-bench-TSS is same as NAS-Bench-201 \cite{dong2020nasbench201}. 
    The architectures in NATS-bench-SSS and NATS-bench-TSS are trained by the three datasets: 
    CIFAR10, CIFAR100, ImageNet-16-120, while NAS-Bench-101 is trained by one: 
    CIFAR10. 

    \subsection{Environment and Parameter Settings}
    The experiment is conducted on a PC with Intel Core i7-11700K (3.60 GHz, 16-MB Cache, and 16 cores), 
    a single NVIDIA RTX3070 Ti GPU with 8 GB memory, driver version 520.61.05, CUDA version 11.8, and 
    65 GB available memory running on Ubuntu 20.04.1 with linux kernel 5.15.0-73-generic. All the program 
    is written in Python 3.7.16 with PyTorch 1.7.1+cu110 package. The simulations of all 
    non-weight sharing NASs are carried out for 500 runs on all the search spaces except AREA, which is 50 runs. 
    For weight sharing NAS algorithms, each is carried out for 3 runs. And for training-free 
    algorithms, each is carried out for 100 runs and are given an evaluation 
    budget of 1000 for all the search space. The parameter settings of \palg{} are as follows: 
    initial temperature, ending temperature, rate of decrease of temperature, the number of iteration, 
    the constant number adjusting acceptance probability, the number of generated neighbours, and 
    the coefficient of the size of historical best architectures set are set 
    to 1, 0.0008, 0.745, 4, 0.25, 10, 1, so that the maximum number of searching candidates is 1000. 
    Notbly, in NAS-Bench-101 search space, we conduct experiments with an evaluation budget of 5000 for RS, 
    since NAS-Bench-101 contains much more architectures.

    \subsection{Simulation Results} % 補實驗描述
    % sss
    The simulation results in NATS-Bench-SSS \cite{Dong_2021} is shown in \xtab{table:overall_sss}. 
    The proposed NAS algorithm and ranking function is compared with the other five non-weight sharing 
    algorithm and three training-free score functions with RS, i.e., RS-NI, RS-NASWOT, 
    and RS-LogSynFlow. For fairness, the searching architectures for the RS with training-free 
    score functions are identical. Among the four training-free score functions with RS, RS-LogSynFlow 
    has the highest accuracy in CIFAR10. The accuracy of RS-NI and RS-rank are similar, but the stability 
    of RS-rank is better. In CIFAR100, RS-LogSynFlow performs best, then is RS-NI, RS-rank, RS-NASWOT. 
    In ImageNet16, RS-LogSynFlow still has the highest accuracy and stability, then is RS-rank, RS-NI, 
    RS-NASWOT. However, \palg{} improves the results significantly and outperforms almost all the 
    compared algorithms, including AREA, REA, and RL. It can be seen that, search algorithms guides 
    the searching direction for searching better architectures, and the training-free score functions
    accelerate the whole searching process by eliminating the training stage. 

    % tss
    In NATS-Bench-TSS \cite{Dong_2021}, \xtab{table:overall_201} compares \palg{} with the other 
    non-weight sharing algorithms, six weight sharing algorithms, three different training-free 
    score functions with RS, i.e., RS-NI, RS-NASWOT, and RS-LogSynFlow and one 
    training-free method, TE-NAS. Same as the experiment of NATS-Bench-SSS, the searching 
    architectures are identical for the RS with score functions. Non-weight sharing algorithms 
    like REA and AREA outperform all the other search algorithms, though the time cost is 
    tremendous. Among the four training-free score functions with random search, RS-rank has the 
    highest accuracy in CIFAR10 and ImageNet16. But in CIFAR100, RS-NI has the highest accuracy. 
    In CIFAR10 validation dataset, RS-NI is the most stable, RS-rank the second, RS-LogSynFlow the 
    third and RS-NASWOT the last. In CIFAR10 test dataset, RS-rank is the most stable, then is RS-NI, 
    RS-LogSynFlow, RS-NASWOT. In CIFAR100, RS-NI is the most stable, RS-NASWOT the second but 
    has lower accuracy, RS-rank the third, the last is RS-LogSynFlow. In ImageNet16, RS-rank is the 
    most stable, then is RS-NI, RS-NASWOT, RS-LogSynFlow. According to the results, RS-rank does 
    integrate the score functions and use them to evaluate an architecture by different characteristics. 
    With the help of search algorithm, \palg{} outperforms all the others training-free methods, 
    and obtains a better stability. In CIFAR10 and CIFAR100, the standard deviation of \palg{} is lower 
    than all the non-weight sharing algorithms. And in ImageNet16, the standard deviation of \palg{} is 
    lower than all the training-free methods. 

    % 101
    The result of the experiment in NAS-Bench-101 \cite{ying2019nasbench101} is shown in \xtab{table:overall_101}. 
    It compares rank function and \palg{} with two non-weight sharing algorithms, two weight 
    sharing algorithms, and three training-free score functions with RS. The training-free 
    score functions are same to the ones in NATS-Bench-TSS and NATS-Bench-SSS. In the experiment, ENAS has 
    the best performance, while the computation time is the highest. AREA and REA performs similarly. 
    Both have higher accuracy than most of others. 
    In CIFAR10 validation dataset, with $1000$ sampled architectures ($N=1000$), the accuracy of 
    RS-NI is better than RS-rank, but the standard deviation is worse than RS-rank. And RS-NASWOT 
    though is higher than RS-LogSynFlow, RS-NASWOT is the most unstable. With $5000$ sampled architectures 
    ($N=5000$), RS-rank outperforms RS-NI instead and becomes the best method. RS-NASWOT and RS-LogSynFlow
    performs similarly, but RS-NASWOT is more unstable. Lastly, \palg{} has better accuracy 
    and stability than all the other training-free score functions. In CIFAR10 test dataset, with $1000$ 
    sampled architectures ($N=1000$), RS-NI has higher accuracy than RS-rank, however, the stability of 
    RS-NI is much worse than RS-rank. The accuracy and stability of \palg{} is the best among the 
    training-free methods. With $5000$ sampled architectures ($N=5000$), RS-NI performs better than 
    RS-rank but has much lower stability. RS-LogSynFlow performs better than RS-NASWOT which not only 
    has low accuracy but also low stability. \palg{} is still best among these training-free score 
    functions. Compared with \palg{} and the other NAS methods, though the performance and stability 
    are a little bit worse than AREA, REA and ENAS, the time cost of \palg{} is much less than them. 

    \begin{table}[t]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-101.}}
        \vspace{-\baselineskip}
        \resizebox{0.5 \textwidth}{!}{
        \begin{tabular}{@{}lrcc@{}}\toprule
            Method              & Search (s) & \multicolumn{2}{c}{CIFAR-10}        \\ \cmidrule(lr){3-4}
                                &            & validation       & test             \\ \midrule
                                &            & \textbf{Non-weight sharing} &       \\
            AREA                & $12,000$   & $93.67 \pm 0.48$ & $93.02 \pm 0.48$ \\
            REA                 & $12,000$   & $93.63 \pm 0.50$ & $93.02 \pm 0.52$ \\ \midrule
                                &            & \textbf{Weight sharing}     &       \\
            DARTS-V1            & $20,483$   & $83.50 \pm 0.11$ & $83.74 \pm 0.03$ \\
            ENAS                & $22,325$   & $93.83 \pm 0.28$ & $93.34 \pm 0.26$ \\ \midrule
                                &            & \textbf{Training-free}      &       \\
            RS-NI (N=1,000)         & $476$  & $92.89 \pm 2.10$ & $92.55 \pm 1.33$ \\ 
            RS-NASWOT (N=1,000)     & $127$  & $92.27 \pm 6.60$ & $92.10 \pm 4.44$ \\ 
            RS-LogSynFlow (N=1,000) & $76$   & $91.81 \pm 1.52$ & $91.21 \pm 1.71$ \\ 
            RS-rank (ours) (N=1000) & $679$  & $92.44 \pm 1.42$ & $91.94 \pm 2.01$ \\
            RS-NI (N=5,000)         & $2390$ & $92.84 \pm 1.55$ & $92.35 \pm 2.69$ \\ 
            RS-NASWOT (N=5,000)     & $635$  & $91.67 \pm 4.99$ & $90.60 \pm 6.64$ \\ 
            RS-LogSynFlow (N=5,000) & $408$  & $91.66 \pm 1.72$ & $91.17 \pm 1.86$ \\ 
            RS-rank (ours) (N=5,000)& $3434$  & $92.87 \pm 0.97$ & $92.32 \pm 1.47$ \\ 
            \palg{} (ours)          & $595$  & $93.16 \pm 0.97$ & $92.77 \pm 1.03$ \\ \midrule
            %SA-NI                  &        & $92.38 \pm 3.50$ & $92.71 \pm 1.97$
            %SA-NASWOT
            Optimal             &            & $95.12$          & $94.32$          \\ \bottomrule
            \end{tabular}
        }
        \label{table:overall_101}
        \vspace{-\baselineskip}
    \end{table}

    \section{Conclusion}
    \label{sec:conclusion}
    In this paper, a novel concept of ``complementary'' is proposed and used as a basis to choose 
    training-free score functions. The new rank algorithm is then applied to simulated annealing 
    algorithm. The simulation result shows that the proposed algorithm, named \palg{}, not only outperforms the most 
    state-of-art training-free score functions but also some of the non-training-free methods. 
    The proposed algorithm also shows robustness, which performs well between different search 
    spaces vary with neural network structures and size. 
    The limitation of \palg{} is the lack of versatility since the score functions are mainly chosen 
    for common image recognition. That means \palg{} may not be applied in the search of architectures 
    designed for other usages, i.e., speech recognition, NLP. However, the problem is able to be solved 
    by choosing the corresponding complementary score functions. Some further questions are how many 
    score functions are required for precisely estimating the performance of architectures and what are they? 
    
    \bibliographystyle{IEEEtran}
    \bibliography{manuscript}
\end{document}

