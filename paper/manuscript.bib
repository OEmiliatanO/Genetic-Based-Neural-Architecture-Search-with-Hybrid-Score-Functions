@misc{https://doi.org/10.48550/arxiv.1611.01578,
  doi = {10.48550/ARXIV.1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2111.13336,
  doi = {10.48550/ARXIV.2111.13336},
  url = {https://arxiv.org/abs/2111.13336},
  author = {Sun, Zhenhong and Lin, Ming and Sun, Xiuyu and Tan, Zhiyu and Li, Hao and Jin, Rong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2006.04647,
  doi = {10.48550/ARXIV.2006.04647},
  url = {https://arxiv.org/abs/2006.04647},
  author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search without Training},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2011.05649,
  doi = {10.48550/ARXIV.2011.05649},
  url = {https://arxiv.org/abs/2011.05649},
  author = {Zheng, Huahuan and An, Keyu and Ou, Zhijian},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Neural Architecture Search for End-to-end Speech Recognition via Straight-Through Gradients},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{jiang-etal-2019-improved,
    title = "Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition",
    author = "Jiang, Yufan  and
      Hu, Chi  and
      Xiao, Tong  and
      Zhang, Chunliang  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1367",
    doi = "10.18653/v1/D19-1367",
    pages = "3585--3590",
    abstract = "In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task.",
}

@misc{https://doi.org/10.48550/arxiv.2102.11535,
  doi = {10.48550/ARXIV.2102.11535},
  url = {https://arxiv.org/abs/2102.11535},
  author = {Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2109.00817,
  doi = {10.48550/ARXIV.2109.00817},
  url = {https://arxiv.org/abs/2109.00817},
  author = {Shu, Yao and Cai, Shaofeng and Dai, Zhongxiang and Ooi, Beng Chin and Low, Bryan Kian Hsiang},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NASI: Label- and Data-agnostic Neural Architecture Search at Initialization},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{https://doi.org/10.48550/arxiv.2110.08616,
  doi = {10.48550/ARXIV.2110.08616},
  url = {https://arxiv.org/abs/2110.08616},
  author = {Zhang, Zhihao and Jia, Zhihao},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GradSign: Model Performance Inference with Theoretical Insights},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2207.05135,
  doi = {10.48550/ARXIV.2207.05135},
  url = {https://arxiv.org/abs/2207.05135},
  author = {Cavagnero, Niccolò and Robbiano, Luca and Caputo, Barbara and Averta, Giuseppe},
  keywords = {Neural and Evolutionary Computing (cs.NE), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FreeREA: Training-Free Evolution-based Architecture Search},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{10030904,
  author={Cavagnero, Niccolò and Robbiano, Luca and Caputo, Barbara and Averta, Giuseppe},
  booktitle={2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={FreeREA: Training-Free Evolution-based Architecture Search}, 
  year={2023},
  volume={},
  number={},
  pages={1493-1502},
  doi={10.1109/WACV56688.2023.00154}
}

@misc{https://doi.org/10.48550/arxiv.1912.13053,
  doi = {10.48550/ARXIV.1912.13053},
  url = {https://arxiv.org/abs/1912.13053},
  author = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S.},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Disentangling Trainability and Generalization in Deep Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Lee_2020,
	doi = {10.1088/1742-5468/abc62b},
	url = {https://doi.org/10.1088%2F1742-5468%2Fabc62b},
	year = 2020,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {2020},
	number = {12},
	pages = {124002},
	author = {Jaehoon Lee and Lechao Xiao and Samuel S Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jeffrey Pennington},
	title = {Wide neural networks of any depth evolve as linear models under gradient descent}, 
	journal = {Journal of Statistical Mechanics: Theory and Experiment}
}

@article{Liu_2023,
	doi = {10.1109/tnnls.2021.3100554},
	url = {https://doi.org/10.1109%2Ftnnls.2021.3100554},
	year = 2023,
	month = {feb},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {34},
	number = {2},
	pages = {550--570},
	author = {Yuqiao Liu and Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen and Kay Chen Tan},
	title = {A Survey on Evolutionary Neural Architecture Search},
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@ARTICLE{10092788,
  author={Wu, Meng-Ting and Lin, Hung-I and Tsai, Chun-Wei},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Training-Free Neural Architecture Search Algorithm based on Search Economics}, 
  year={2023},
  volume={},
  number={},
  pages={},
  doi={10.1109/TEVC.2023.3264533}
}
@misc{tanaka2020pruning,
      title={Pruning neural networks without any data by iteratively conserving synaptic flow}, 
      author={Hidenori Tanaka and Daniel Kunin and Daniel L. K. Yamins and Surya Ganguli},
      year={2020},
      eprint={2006.05467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{abdelfattah2021zerocost,
      title={Zero-Cost Proxies for Lightweight NAS}, 
      author={Mohamed S. Abdelfattah and Abhinav Mehrotra and Łukasz Dudziak and Nicholas D. Lane},
      year={2021},
      eprint={2101.08134},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Cavagnero_2023,
	doi = {10.1109/wacv56688.2023.00154},
	url = {https://doi.org/10.1109%2Fwacv56688.2023.00154},
	year = 2023,
	month = {jan},
	publisher = {{IEEE}},
	author = {Niccolo Cavagnero and Luca Robbiano and Barbara Caputo and Giuseppe Averta},
	title = {{FreeREA}: Training-Free Evolution-based Architecture Search},
	booktitle = {2023 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})}
}

@article{Dong_2021,
	doi = {10.1109/tpami.2021.3054824},
	url = {https://doi.org/10.1109%2Ftpami.2021.3054824},
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	pages = {},
	author = {Xuanyi Dong and Lu Liu and Katarzyna Musial and Bogdan Gabrys},
	title = {{NATS}-Bench: Benchmarking {NAS} Algorithms for Architecture Topology and Size},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}
@article{kirkpatrick1983optimization,
  title={Optimization by simulated annealing},
  author={Kirkpatrick, Scott and Gelatt Jr, C Daniel and Vecchi, Mario P},
  journal={science},
  volume={220},
  number={4598},
  pages={671--680},
  year={1983},
  publisher={American association for the advancement of science}
}
@book{mitchell1998introduction,
  title={An introduction to genetic algorithms},
  author={Mitchell, Melanie},
  year={1998},
  publisher={MIT press}
}
@misc{wang2020picking,
      title={Picking Winning Tickets Before Training by Preserving Gradient Flow}, 
      author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
      year={2020},
      eprint={2002.07376},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lee2019snip,
      title={SNIP: Single-shot Network Pruning based on Connection Sensitivity}, 
      author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
      year={2019},
      eprint={1810.02340},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{7379579,
  author={Tsai, Chun-Wei},
  booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 
  title={Search Economics: A Solution Space and Computing Resource Aware Search Method}, 
  year={2015},
  volume={},
  number={},
  pages={2555-2560},
  doi={10.1109/SMC.2015.447}
}
@incollection{Lopes_2021,
	doi = {10.1007/978-3-030-86383-8_44},
	url = {https://doi.org/10.1007%2F978-3-030-86383-8_44},
	year = 2021,
	publisher = {Springer International Publishing},
	pages = {552--563},
	author = {Vasco Lopes and Saeid Alirezazadeh and Lu{\'{\i}}s A. Alexandre},
	title = {{EPE}-{NAS}: Efficient Performance Estimation Without Training for Neural Architecture Search},
	booktitle = {Lecture Notes in Computer Science}
}

@misc{dong2020nasbench201,
      title={NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search}, 
      author={Xuanyi Dong and Yi Yang},
      year={2020},
      eprint={2001.00326},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ying2019nasbench101,
      title={NAS-Bench-101: Towards Reproducible Neural Architecture Search}, 
      author={Chris Ying and Aaron Klein and Esteban Real and Eric Christiansen and Kevin Murphy and Frank Hutter},
      year={2019},
      eprint={1902.09635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zoph2017neural,
      title={Neural Architecture Search with Reinforcement Learning}, 
      author={Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1611.01578},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{10.5909/JBE.2020.26.7.855,
  author    = {TranLinh Tam and BaeSung-Ho},
  title     = {Training-Free Hardware-Aware Neural Architecture Search with Reinforcement Learning},
  journal   = {Journal of Broadcast Engineering},
  publisher = {한국방송∙미디어공학회},
  volume    = {26},
  number    = {7},
  pages     = {855-861},
  year      = {2021},
  month     = {12}
}

@inproceedings{Dong_2019,
	doi = {10.1109/iccv.2019.00378},
	url = {https://doi.org/10.1109%2Ficcv.2019.00378},
	year = {2019},
	month = {oct},
	publisher = {{IEEE}},
	author = {Xuanyi Dong and Yi Yang},
	title = {One-Shot Neural Architecture Search via Self-Evaluated Template Network},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})}
}

@inproceedings{10.1145/3491396.3506510,
  author = {Wu, Meng-Ting and Lin, Hung-I and Tsai, Chun-Wei},
  title = {A Training-Free Genetic Neural Architecture Search},
  year = {2022},
  isbn = {9781450391603},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3491396.3506510},
  doi = {10.1145/3491396.3506510},
  abstract = {The so-called neural architecture search (NAS) provides an alternative way to construct a "good neural architecture," which would normally outperform hand-made architectures, for solving complex problems without domain knowledge. However, a critical issue for most of the NAS techniques is in that it is computationally very expensive because several complete/partial training processes are involved in evaluating the goodness of a neural architecture during the process of NAS. To mitigate this problem for evaluating a single neural architecture found by the search algorithm of NAS, we present an efficient NAS in this study, called genetic algorithm and noise immunity for neural architecture search without training (GA-NINASWOT). The genetic algorithm (GA) in the proposed algorithm is used to search for high potential neural architectures while a modified scoring method based on the neural architecture search without training (NASWOT) is used to replace the training process of each neural architecture found by the GA for measuring its quality. To evaluate the performance of GA-NINASWOT, we compared it with several state-of-the-art NAS techniques, which include weight-sharing methods, non-weight-sharing methods, and NASWOT. Simulation results show that GA-NINASWOT outperforms all the other state-of-the-art weight-sharing methods and NASWOT compared in this study in terms of the accuracy and computational time. Moreover, GA-NINASWOT gives a result that is comparable to those found by the non-weight-sharing methods while reducing 99% of the search time.},
  booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
  pages = {65–70},
  numpages = {6},
  keywords = {training-free, genetic algorithm, Neural architecture search},
  location = {Jinan, China},
  series = {ACM ICEA '21}
}

@misc{lin2021zennas,
      title={Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition}, 
      author={Ming Lin and Pichao Wang and Zhenhong Sun and Hesen Chen and Xiuyu Sun and Qi Qian and Hao Li and Rong Jin},
      year={2021},
      eprint={2102.01063},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{sun2022maedet,
      title={MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection}, 
      author={Zhenhong Sun and Ming Lin and Xiuyu Sun and Zhiyu Tan and Hao Li and Rong Jin},
      year={2022},
      eprint={2111.13336},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jacot2020neural,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search}, 
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{falkner2018bohb,
      title={BOHB: Robust and Efficient Hyperparameter Optimization at Scale}, 
      author={Stefan Falkner and Aaron Klein and Frank Hutter},
      year={2018},
      eprint={1807.01774},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2019random,
      title={Random Search and Reproducibility for Neural Architecture Search}, 
      author={Liam Li and Ameet Talwalkar},
      year={2019},
      eprint={1902.07638},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu2019darts,
      title={DARTS: Differentiable Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
      year={2019},
      eprint={1806.09055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dong2019searching,
      title={Searching for A Robust Neural Architecture in Four GPU Hours}, 
      author={Xuanyi Dong and Yi Yang},
      year={2019},
      eprint={1910.04465},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{pham2018efficient,
      title={Efficient Neural Architecture Search via Parameter Sharing}, 
      author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
      year={2018},
      eprint={1802.03268},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
