@misc{https://doi.org/10.48550/arxiv.1611.01578,
  doi = {10.48550/ARXIV.1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2111.13336,
  doi = {10.48550/ARXIV.2111.13336},
  url = {https://arxiv.org/abs/2111.13336},
  author = {Sun, Zhenhong and Lin, Ming and Sun, Xiuyu and Tan, Zhiyu and Li, Hao and Jin, Rong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2006.04647,
  doi = {10.48550/ARXIV.2006.04647},
  url = {https://arxiv.org/abs/2006.04647},
  author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search without Training},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2011.05649,
  doi = {10.48550/ARXIV.2011.05649},
  url = {https://arxiv.org/abs/2011.05649},
  author = {Zheng, Huahuan and An, Keyu and Ou, Zhijian},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Neural Architecture Search for End-to-end Speech Recognition via Straight-Through Gradients},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{mehrotra2021nasbenchasr,
title={{\{}NAS{\}}-Bench-{\{}ASR{\}}: Reproducible Neural Architecture Search for Speech Recognition},
author={Abhinav Mehrotra and Alberto Gil C. P. Ramos and Sourav Bhattacharya and {\L}ukasz Dudziak and Ravichander Vipperla and Thomas Chau and Mohamed S Abdelfattah and Samin Ishtiaq and Nicholas Donald Lane},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=CU0APx9LMaL}
}

@inproceedings{jiang-etal-2019-improved,
    title = "Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition",
    author = "Jiang, Yufan  and
      Hu, Chi  and
      Xiao, Tong  and
      Zhang, Chunliang  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1367",
    doi = "10.18653/v1/D19-1367",
    pages = "3585--3590",
    abstract = "In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task.",
}

@misc{https://doi.org/10.48550/arxiv.2006.07116,
  doi = {10.48550/ARXIV.2006.07116},
  url = {https://arxiv.org/abs/2006.07116},
  author = {Klyuchnikov, Nikita and Trofimov, Ilya and Artemova, Ekaterina and Salnikov, Mikhail and Fedorov, Maxim and Burnaev, Evgeny},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2005.14187,
  doi = {10.48550/ARXIV.2005.14187},
  url = {https://arxiv.org/abs/2005.14187},
  author = {Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {HAT: Hardware-Aware Transformers for Efficient Natural Language Processing},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2102.11535,
  doi = {10.48550/ARXIV.2102.11535},
  url = {https://arxiv.org/abs/2102.11535},
  author = {Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2203.09137,
  doi = {10.48550/ARXIV.2203.09137},
  url = {https://arxiv.org/abs/2203.09137},
  author = {Wang, Haoxiang and Wang, Yite and Sun, Ruoyu and Li, Bo},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2109.00817,
  doi = {10.48550/ARXIV.2109.00817},
  url = {https://arxiv.org/abs/2109.00817},
  author = {Shu, Yao and Cai, Shaofeng and Dai, Zhongxiang and Ooi, Beng Chin and Low, Bryan Kian Hsiang},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NASI: Label- and Data-agnostic Neural Architecture Search at Initialization},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{https://doi.org/10.48550/arxiv.2110.08616,
  doi = {10.48550/ARXIV.2110.08616},
  url = {https://arxiv.org/abs/2110.08616},
  author = {Zhang, Zhihao and Jia, Zhihao},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GradSign: Model Performance Inference with Theoretical Insights},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2207.05135,
  doi = {10.48550/ARXIV.2207.05135},
  url = {https://arxiv.org/abs/2207.05135},
  author = {Cavagnero, Niccol√≤ and Robbiano, Luca and Caputo, Barbara and Averta, Giuseppe},
  keywords = {Neural and Evolutionary Computing (cs.NE), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FreeREA: Training-Free Evolution-based Architecture Search},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2101.08134,
  doi = {10.48550/ARXIV.2101.08134},
  url = {https://arxiv.org/abs/2101.08134},
  author = {Abdelfattah, Mohamed S. and Mehrotra, Abhinav and Dudziak, Lukasz and Lane, Nicholas D.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Zero-Cost Proxies for Lightweight NAS},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@inproceedings{10.1145/3491396.3506510,
  author = {Wu, Meng-Ting and Lin, Hung-I and Tsai, Chun-Wei},
  title = {A Training-Free Genetic Neural Architecture Search},
  year = {2022},
  isbn = {9781450391603},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3491396.3506510},
  doi = {10.1145/3491396.3506510},
  abstract = {The so-called neural architecture search (NAS) provides an alternative way to construct a "good neural architecture," which would normally outperform hand-made architectures, for solving complex problems without domain knowledge. However, a critical issue for most of the NAS techniques is in that it is computationally very expensive because several complete/partial training processes are involved in evaluating the goodness of a neural architecture during the process of NAS. To mitigate this problem for evaluating a single neural architecture found by the search algorithm of NAS, we present an efficient NAS in this study, called genetic algorithm and noise immunity for neural architecture search without training (GA-NINASWOT). The genetic algorithm (GA) in the proposed algorithm is used to search for high potential neural architectures while a modified scoring method based on the neural architecture search without training (NASWOT) is used to replace the training process of each neural architecture found by the GA for measuring its quality. To evaluate the performance of GA-NINASWOT, we compared it with several state-of-the-art NAS techniques, which include weight-sharing methods, non-weight-sharing methods, and NASWOT. Simulation results show that GA-NINASWOT outperforms all the other state-of-the-art weight-sharing methods and NASWOT compared in this study in terms of the accuracy and computational time. Moreover, GA-NINASWOT gives a result that is comparable to those found by the non-weight-sharing methods while reducing 99% of the search time.},
  booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
  pages = {65‚Äì70},
  numpages = {6},
  keywords = {training-free, genetic algorithm, Neural architecture search},
  location = {Jinan, China},
  series = {ACM ICEA '21}
}

@misc{https://doi.org/10.48550/arxiv.1912.13053,
  doi = {10.48550/ARXIV.1912.13053},
  url = {https://arxiv.org/abs/1912.13053},
  author = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S.},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Disentangling Trainability and Generalization in Deep Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Lee_2020,
	doi = {10.1088/1742-5468/abc62b},
	url = {https://doi.org/10.1088%2F1742-5468%2Fabc62b},
	year = 2020,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {2020},
	number = {12},
	pages = {124002},
	author = {Jaehoon Lee and Lechao Xiao and Samuel S Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jeffrey Pennington},
	title = {Wide neural networks of any depth evolve as linear models under gradient descent}, 
	journal = {Journal of Statistical Mechanics: Theory and Experiment}
}

@article{Liu_2023,
	doi = {10.1109/tnnls.2021.3100554},
	url = {https://doi.org/10.1109%2Ftnnls.2021.3100554},
	year = 2023,
	month = {feb},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {34},
	number = {2},
	pages = {550--570},
	author = {Yuqiao Liu and Yanan Sun and Bing Xue and Mengjie Zhang and Gary G. Yen and Kay Chen Tan},
	title = {A Survey on Evolutionary Neural Architecture Search},
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@ARTICLE{10092788,
  author={Wu, Meng-Ting and Lin, Hung-I and Tsai, Chun-Wei},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Training-Free Neural Architecture Search Algorithm based on Search Economics}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TEVC.2023.3264533}
}
@misc{tanaka2020pruning,
      title={Pruning neural networks without any data by iteratively conserving synaptic flow}, 
      author={Hidenori Tanaka and Daniel Kunin and Daniel L. K. Yamins and Surya Ganguli},
      year={2020},
      eprint={2006.05467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{abdelfattah2021zerocost,
      title={Zero-Cost Proxies for Lightweight NAS}, 
      author={Mohamed S. Abdelfattah and Abhinav Mehrotra and ≈Åukasz Dudziak and Nicholas D. Lane},
      year={2021},
      eprint={2101.08134},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Cavagnero_2023,
	doi = {10.1109/wacv56688.2023.00154},
	url = {https://doi.org/10.1109%2Fwacv56688.2023.00154},
	year = 2023,
	month = {jan},
	publisher = {{IEEE}},
	author = {Niccolo Cavagnero and Luca Robbiano and Barbara Caputo and Giuseppe Averta},
	title = {{FreeREA}: Training-Free Evolution-based Architecture Search},
	booktitle = {2023 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})}
}

@article{Dong_2021,
	doi = {10.1109/tpami.2021.3054824},
	url = {https://doi.org/10.1109%2Ftpami.2021.3054824},
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	pages = {1--1},
	author = {Xuanyi Dong and Lu Liu and Katarzyna Musial and Bogdan Gabrys},
	title = {{NATS}-Bench: Benchmarking {NAS} Algorithms for Architecture Topology and Size},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}
@article{kirkpatrick1983optimization,
  title={Optimization by simulated annealing},
  author={Kirkpatrick, Scott and Gelatt Jr, C Daniel and Vecchi, Mario P},
  journal={science},
  volume={220},
  number={4598},
  pages={671--680},
  year={1983},
  publisher={American association for the advancement of science}
}
@book{mitchell1998introduction,
  title={An introduction to genetic algorithms},
  author={Mitchell, Melanie},
  year={1998},
  publisher={MIT press}
}
@misc{wang2020picking,
      title={Picking Winning Tickets Before Training by Preserving Gradient Flow}, 
      author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
      year={2020},
      eprint={2002.07376},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lee2019snip,
      title={SNIP: Single-shot Network Pruning based on Connection Sensitivity}, 
      author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
      year={2019},
      eprint={1810.02340},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{frankle2019lottery,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{7379579,
  author={Tsai, Chun-Wei},
  booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 
  title={Search Economics: A Solution Space and Computing Resource Aware Search Method}, 
  year={2015},
  volume={},
  number={},
  pages={2555-2560},
  doi={10.1109/SMC.2015.447}
}
@incollection{Lopes_2021,
	doi = {10.1007/978-3-030-86383-8_44},
  
	url = {https://doi.org/10.1007%2F978-3-030-86383-8_44},
  
	year = 2021,
	publisher = {Springer International Publishing},
  
	pages = {552--563},
  
	author = {Vasco Lopes and Saeid Alirezazadeh and Lu{\'{\i}}s A. Alexandre},
  
	title = {{EPE}-{NAS}: Efficient Performance Estimation Without Training for Neural Architecture Search},
  
	booktitle = {Lecture Notes in Computer Science}
}