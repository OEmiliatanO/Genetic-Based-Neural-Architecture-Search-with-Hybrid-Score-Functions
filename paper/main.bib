@misc{https://doi.org/10.48550/arxiv.1611.01578,
  doi = {10.48550/ARXIV.1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search with Reinforcement Learning},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2111.13336,
  doi = {10.48550/ARXIV.2111.13336},
  url = {https://arxiv.org/abs/2111.13336},
  author = {Sun, Zhenhong and Lin, Ming and Sun, Xiuyu and Tan, Zhiyu and Li, Hao and Jin, Rong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2006.04647,
  doi = {10.48550/ARXIV.2006.04647},
  url = {https://arxiv.org/abs/2006.04647},
  author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search without Training},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2011.05649,
  doi = {10.48550/ARXIV.2011.05649},
  url = {https://arxiv.org/abs/2011.05649},
  author = {Zheng, Huahuan and An, Keyu and Ou, Zhijian},
  keywords = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Neural Architecture Search for End-to-end Speech Recognition via Straight-Through Gradients},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{mehrotra2021nasbenchasr,
title={{\{}NAS{\}}-Bench-{\{}ASR{\}}: Reproducible Neural Architecture Search for Speech Recognition},
author={Abhinav Mehrotra and Alberto Gil C. P. Ramos and Sourav Bhattacharya and {\L}ukasz Dudziak and Ravichander Vipperla and Thomas Chau and Mohamed S Abdelfattah and Samin Ishtiaq and Nicholas Donald Lane},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=CU0APx9LMaL}
}

@inproceedings{jiang-etal-2019-improved,
    title = "Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition",
    author = "Jiang, Yufan  and
      Hu, Chi  and
      Xiao, Tong  and
      Zhang, Chunliang  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1367",
    doi = "10.18653/v1/D19-1367",
    pages = "3585--3590",
    abstract = "In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task.",
}

@misc{https://doi.org/10.48550/arxiv.2006.07116,
  doi = {10.48550/ARXIV.2006.07116},
  url = {https://arxiv.org/abs/2006.07116},
  author = {Klyuchnikov, Nikita and Trofimov, Ilya and Artemova, Ekaterina and Salnikov, Mikhail and Fedorov, Maxim and Burnaev, Evgeny},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2005.14187,
  doi = {10.48550/ARXIV.2005.14187},
  url = {https://arxiv.org/abs/2005.14187},
  author = {Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {HAT: Hardware-Aware Transformers for Efficient Natural Language Processing},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1805.07159,
  doi = {10.48550/ARXIV.1805.07159},
  url = {https://arxiv.org/abs/1805.07159},
  author = {Camero, Andrés and Toutouh, Jamal and Alba, Enrique},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Low-Cost Recurrent Neural Network Expected Performance Evaluation},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Camero_2021,
	doi = {10.1016/j.asoc.2021.107356},
	url = {https://doi.org/10.1016%2Fj.asoc.2021.107356},
	year = 2021,
	month = {jul},
	publisher = {Elsevier {BV}},
	volume = {106},
	pages = {107356},
	author = {Andr{\'{e}}s Camero and Hao Wang and Enrique Alba and Thomas Bäck},
	title = {Bayesian neural architecture search using a training-free performance metric},
	journal = {Applied Soft Computing}
}

@misc{https://doi.org/10.48550/arxiv.2106.15295,
  doi = {10.48550/ARXIV.2106.15295},
  url = {https://arxiv.org/abs/2106.15295},
  author = {Camero, Andrés and Toutouh, Jamal and Alba, Enrique},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Reliable and Fast Recurrent Neural Network Architecture Optimization},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2102.11535,
  doi = {10.48550/ARXIV.2102.11535},
  url = {https://arxiv.org/abs/2102.11535},
  author = {Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2203.09137,
  doi = {10.48550/ARXIV.2203.09137},
  url = {https://arxiv.org/abs/2203.09137},
  author = {Wang, Haoxiang and Wang, Yite and Sun, Ruoyu and Li, Bo},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2109.00817,
  doi = {10.48550/ARXIV.2109.00817},
  url = {https://arxiv.org/abs/2109.00817},
  author = {Shu, Yao and Cai, Shaofeng and Dai, Zhongxiang and Ooi, Beng Chin and Low, Bryan Kian Hsiang},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NASI: Label- and Data-agnostic Neural Architecture Search at Initialization},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{https://doi.org/10.48550/arxiv.2110.08616,
  doi = {10.48550/ARXIV.2110.08616},
  url = {https://arxiv.org/abs/2110.08616},
  author = {Zhang, Zhihao and Jia, Zhihao},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GradSign: Model Performance Inference with Theoretical Insights},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2207.05135,
  doi = {10.48550/ARXIV.2207.05135},
  url = {https://arxiv.org/abs/2207.05135},
  author = {Cavagnero, Niccolò and Robbiano, Luca and Caputo, Barbara and Averta, Giuseppe},
  keywords = {Neural and Evolutionary Computing (cs.NE), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FreeREA: Training-Free Evolution-based Architecture Search},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2101.08134,
  doi = {10.48550/ARXIV.2101.08134},
  url = {https://arxiv.org/abs/2101.08134},
  author = {Abdelfattah, Mohamed S. and Mehrotra, Abhinav and Dudziak, Lukasz and Lane, Nicholas D.},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Zero-Cost Proxies for Lightweight NAS},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@inproceedings{10.1145/3491396.3506510,
  author = {Wu, Meng-Ting and Lin, Hung-I and Tsai, Chun-Wei},
  title = {A Training-Free Genetic Neural Architecture Search},
  year = {2022},
  isbn = {9781450391603},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3491396.3506510},
  doi = {10.1145/3491396.3506510},
  abstract = {The so-called neural architecture search (NAS) provides an alternative way to construct a "good neural architecture," which would normally outperform hand-made architectures, for solving complex problems without domain knowledge. However, a critical issue for most of the NAS techniques is in that it is computationally very expensive because several complete/partial training processes are involved in evaluating the goodness of a neural architecture during the process of NAS. To mitigate this problem for evaluating a single neural architecture found by the search algorithm of NAS, we present an efficient NAS in this study, called genetic algorithm and noise immunity for neural architecture search without training (GA-NINASWOT). The genetic algorithm (GA) in the proposed algorithm is used to search for high potential neural architectures while a modified scoring method based on the neural architecture search without training (NASWOT) is used to replace the training process of each neural architecture found by the GA for measuring its quality. To evaluate the performance of GA-NINASWOT, we compared it with several state-of-the-art NAS techniques, which include weight-sharing methods, non-weight-sharing methods, and NASWOT. Simulation results show that GA-NINASWOT outperforms all the other state-of-the-art weight-sharing methods and NASWOT compared in this study in terms of the accuracy and computational time. Moreover, GA-NINASWOT gives a result that is comparable to those found by the non-weight-sharing methods while reducing 99% of the search time.},
  booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
  pages = {65–70},
  numpages = {6},
  keywords = {training-free, genetic algorithm, Neural architecture search},
  location = {Jinan, China},
  series = {ACM ICEA '21}
}
