In the last few years, Artificial Neural Network (ANN) has been used in a variety of fields, i.e., image classification, speech recognition, and Natural Language Processing (NLP).
Most of the architectures of ANN are hand-designed by experts, consuming lots of time and resource. This problem induce the researches of Neural Architecture Search (NAS) algorithm, providing an automated, fast, and accurate approach to build a good architecture of ANN.
However, the NAS algorithm in the past is time-consuming, which is caused by the fact that, during the searching, a candidate of architectures must be trained to know how the performance of this architecture. Thus, plenty of training-free NAS algorithm are developed in recent years. By using training-free score function to evaluate an architecture, there's no requirement for training architectures anymore. Compare with training-need NAS algorithm, training-free NAS algorithm tend to be faster and consuming less resource.
Neverthesless, the low correlation between single Training-Free Score Function and the performance of an architecture limits the performance of training-free NAS algorithms. To mitigate this problem, we propose a genetic-based training-free NAS algorithm with hybrid training-free score function, which combines three highly heterogeneous training-free score functions to evaluate an architecture. To evaluate the performance of the proposed algorithm, we compared it with several NAS algorithms, including weight-sharing methods, non-weight-sharing methods, and NASWOT. We expect develope a faster and more accurate training-free NAS algorithm.