1. introduction
Neural architecture search (NAS) has recently drawn a big amount of attention, since the ability to automatically design a "good" neural architecture. By leveraging machine learning algorithms (Zoph & Le, 2016[https://arxiv.org/abs/1611.01578]), NAS algorithms can explore a search space, which is comprised of numerous potential architectures, to find out a good architectures that outperform those designed by human experts. In recent years, the use of NAS is widespread, from object detection (Sun et al., 2021[https://arxiv.org/abs/2111.13336]), image recognition (Mellor et al., 2020[https://arxiv.org/abs/2006.04647]) and speech recognition (Zheng et al., 2020[https://arxiv.org/abs/2011.05649], Mehrotra et al., 2021[https://openreview.net/forum?id=CU0APx9LMaL]) to natural language processing(Jiang et al., 2019[https://aclanthology.org/D19-1367/], Klyuchnikov et al., 2020[https://arxiv.org/abs/2006.07116], Wang et al., 2020[https://arxiv.org/abs/2005.14187]).

Despite the promising results of NAS, there are still many challenges to conquer. One of the major problems is the extremely high computational cost to search for an optimal architecture, which can make NAS impractical for real-world applications, particularly on resource-constrained platforms like embedded system. The reason why NAS is costly is that during the searching, a candidate architecture must be trained to evaluate how good of this architecture.

To overcome this challenge, recent works developed and proposed lots of method which is so called training-free NAS. Some use mean absolute error random sampling (MRS) (Camero et al., 2018[https://arxiv.org/abs/1805.07159], Camero et al., 2020[https://arxiv.org/abs/2001.10726], Camero et al., 2021[https://arxiv.org/abs/2106.15295]) as a training-free score function and some use neural tangent kernel (NTK) (Chen et al., 2021[https://arxiv.org/abs/2102.11535], Wang et al., 2022[https://arxiv.org/abs/2203.09137], Shu et al., 2021[https://arxiv.org/abs/2109.00817]), while some use gradient-based methods (Zhang & Jia, 2021[https://arxiv.org/abs/2110.08616], Cavagnero et al., 2022[https://arxiv.org/abs/2207.05135], Abdelfattah et al., 2021[https://arxiv.org/abs/2101.08134]).

2. what is unknown?
However, most of score functions suffer from low correlation between score value and the result of an architecture, leading to a trap that no matter how good the search method is used, we can hardly find an optimal architecture.

3. how and why
The major problem causes the low correlation is that a single score function can only evaluate one perspect/characteristic of an architecture.

4. method
By cooperating three heterogeneous score functions with genetic-based search method, we shall evaulate an architecture from different aspects. More specifically, the first score function is noise immunity for neural architecture search without search (NINASWOT) (Wu et al. 2021[https://dl.acm.org/doi/abs/10.1145/3491396.3506510]), as an evaluation of pattern recognition ability. Second one is maximum-entropy detection (MAE-DET) (Sun et al. 2021[https://arxiv.org/abs/2111.13336]), as an evaluation of the entropy of an architecture. Third one is condition number of neural tangent kernel (NTK) (Chen et al. 2021[https://arxiv.org/abs/2102.11535]), as an evaluation of the trainability.

5. results
It's shall find good architectures in reasonable time and computational resource.

6. conclusion
