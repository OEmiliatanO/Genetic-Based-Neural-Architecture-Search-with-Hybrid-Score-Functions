\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%\documentclass[sigconf]{acmart}
%\let\Bbbk\relax %% fix bug
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage{indentfirst}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsopn}
\usepackage{algpseudocode}

\title{GA-based Training-Free NAS Algorithm with Hybrid Score Function}
\author{Hsieh Cheng-Han}
\date{2023}

\begin{document}

\maketitle

\section{Abstract}

    Most neural architecture searches (NASs) are time-consuming caused by 
    the fact that, during the searching, a candidate architecture must be 
    trained to evaluate how good of this architecture. This is why some of 
    training-free NAS algorithms have been proposed in recent years.

    Although the training-free NASs are typically faster than training-based 
    NAS method, however, the correlation between score value and the result 
    of an architecture is not well enough in most cases.
    
    To address this problem, we propose a genetic-based training-free NAS 
    algorithm with hybrid training-free score function, which combines three 
    highly heterogeneous training-free score functions to evaluate an architecture. 
    In this method, the genetic algorithm plays a role to guide the searches 
    of NAS algorithm while the hybrid training-free score function plays the 
    role to evaluate a new candidate architecture during the convergencr process 
    of GA. More precisely, the first score function is noise immunity for 
    neural architecture search without search (NINASWOT), as an evaluation of 
    pattern recognition ability, second one is maximum-entropy detection (MAE-DET), 
    as an evaluation of the entropy of an architecture and the third one is 
    condition number of neural tangent kernel (NTK), as an evaluation of 
    the speed of converge.
    
    To evaluate the performance of the proposed algorithm, we compared it 
    with several NAS algorithms, including weight-sharing methods, 
    non-weight-sharing methods, and neural architecture search without training (NASWOT). 
    We expect develope a faster and more accurate training-free NAS algorithm.

\section{Introduction}

% introduction
    Neural architecture search (NAS) has recently drawn a big amount of 
    attention, since the ability to automatically design a "good" neural 
    architecture. By leveraging machine learning algorithms (Zoph \& Le,
    \ \cite{https://doi.org/10.48550/arxiv.1611.01578}), NAS algorithms 
    can explore a search space, which is comprised of numerous potential 
    architectures, to find out a good architectures that outperform those 
    designed by human experts. In recent years, the use of NAS is widespread, 
    from object detection (Sun et al.,\ \cite{https://doi.org/10.48550/arxiv.2111.13336}), 
    image recognition (Mellor et al.,\ \cite{https://doi.org/10.48550/arxiv.2006.04647}) 
    and speech recognition (Zheng et al.,\ \cite{https://doi.org/10.48550/arxiv.2011.05649}, 
    Mehrotra et al.,\ \cite{mehrotra2021nasbenchasr}) to natural language 
    processing(Jiang et al.,\ \cite{jiang-etal-2019-improved}, 
    Klyuchnikov et al.,\ \cite{https://doi.org/10.48550/arxiv.2006.07116}, 
    Wang et al.,\ \cite{https://doi.org/10.48550/arxiv.2005.14187}).

    Despite the promising results of NAS, there are still many challenges 
    to conquer. One major problem is the extremely high computational 
    cost to search for an optimal architecture, which can make NAS impractical 
    for real-world applications, particularly on resource-constrained 
    platforms like embedded system. The reason why NAS is costly is that 
    during the searching, a candidate architecture must be trained to 
    evaluate how good of this architecture.

    To overcome this challenge, recent works developed and proposed lots 
    of method which is so called training-free NAS. Some use mean absolute 
    error random sampling (MRS) (Camero et al.,\ \cite{https://doi.org/10.48550/arxiv.1805.07159}, 
    Camero et al.,\ \cite{Camero_2021}, Camero et al.,\ \cite{https://doi.org/10.48550/arxiv.2106.15295}) 
    as a training-free score function and some use neural tangent kernel 
    (NTK) (Chen et al.,\ \cite{https://doi.org/10.48550/arxiv.2102.11535}, 
    Wang et al.,\ \cite{https://doi.org/10.48550/arxiv.2203.09137}, 
    Shu et al.,\ \cite{https://doi.org/10.48550/arxiv.2109.00817}), while 
    some use gradient-based methods (Zhang \& Jia,\ \cite{https://doi.org/10.48550/arxiv.2110.08616}, 
    Cavagnero et al.,\ \cite{https://doi.org/10.48550/arxiv.2207.05135}, 
    Abdelfattah et al.,\ \cite{https://doi.org/10.48550/arxiv.2101.08134}).

% what is unknown?
    However, most of score functions suffer from low correlation between 
    score value and the result of an architecture, leading to a predicament 
    that no matter how good the search method is used, we can hardly find 
    an optimal architecture. 
    The major problem causes the low correlation is that a single score 
    function can only evaluate one perspect/characteristic of an architecture.
    
% method
    Based on the fact above, we propose cooperating three heterogeneous score 
    functions with genetic-based search method, we shall evaulate an architecture 
    from different aspects. More specifically, the first score function is 
    noise immunity for neural architecture search without training (NINASWOT) 
    (Wu et al.\ \cite{10.1145/3491396.3506510}). Starting with the work of Mellor 
    et al., they proposed the measurement of the correlation between binary 
    activation paterns of input data at each ReLU layer, named as neural architecture 
    search without training (NASWOT). Later, Wu et al. found a high score obtained 
    by such a function may not correspond to a high-performence model. Thus, 
    they additionally applied noisy immunity method on NASWOT, named as NINASWOT. 
    Second one is maximum-entropy detection (MAE-DET) (Sun et al.\ \cite{https://doi.org/10.48550/arxiv.2111.13336}). 
    Based on the work of Sun et al., they proposed to find the maximum of 
    the differential entropy by calculating the variance of the output at 
    the final layer with the input Gaussian noise images and random weights on 
    Gaussian distribution. Third one is condition number of neural tangent kernel 
    (NTK) (Chen et al.\ \cite{https://doi.org/10.48550/arxiv.2102.11535}), which is 
    defined as 
    \begin{equation}
        \mathcal{K_N}=\frac{\lambda_{\textrm{max}}(\hat\Theta)}{\lambda_{\textrm{min}}(\hat\Theta)}
    \end{equation}
<<<<<<< HEAD
    Chen et al. used the condition number of NTK as the score function to 
    estimate the performance of a network.
% results
    It's shall find good architectures in reasonable time and computational 
    resource.
% conclusion
    The major contribution of this paper can be summarized as follow:
    \begin{itemize}
        \item Use hybrid score functions method to predict the performance 
        of an architecture
        \item 
        \item The simulation results show that 
    \end{itemize}

    The remainder of this paper is organized as follows: Section 2 provides a 
    brief introduction to state-of-the-art NAS techniques and describe the 
    method in detail. Section 3 gives a detailed descriptions of the proposed 
    method. Section 4 provides the results of the simulation.
=======
    Chen et al. used the condition number of NTK as the score to estimate the performance of an architecture.

% results
    The simulation results on cifar-10, cifar-100, and ImageNet-16-120 
    shall show that the proposed method can find good architectures in 
    reasonable time and computational resource. (TODO)

% conclusion
    The major contribution of this paper can be summarized as follow:
    \begin{itemize}
        \item Develope a genetic-based neural architecture search method based on three hybrid score function.
        \item (TODO)
    \end{itemize}

    The remainder of this paper is organized as follows: Section 2 provides the 
    detail about the three heterogeneous score functions. Section 3 gives a detailed 
    description about the proposed method. Section 4 provide the simulation results 
    in different search space. The conclusion and further prospect are gived in Section 5.
>>>>>>> 43e479a2b0dd8d55e1be069a6354b2b9803c3bca

    \bibliographystyle{IEEEtran}
    \bibliography{main}
\end{document}
