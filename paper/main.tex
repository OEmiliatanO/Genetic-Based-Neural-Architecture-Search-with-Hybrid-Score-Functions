%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass[sigconf]{acmart}
\let\Bbbk\relax %% fix bug
\usepackage[utf8]{inputenc}

% =======================
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsopn}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{textcomp}

\usepackage{boxedminipage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{url}
\usepackage{times}
\usepackage{version}
% \usepackage[pdftex]{graphicx}
\usepackage{epsfig}
\usepackage{epsf}
%\usepackage{graphics}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\PassOptionsToPackage{bookmarks={false}}{hyperref}
%%%%%%%%%%%%
\usepackage{comment}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{dblfloatfix}
% ==========================
\input lstset.tex
\input macros.tex

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}

\title{GA-based Training-Free NAS Algorithm with Hybrid Score Function}

\author{Hsieh Cheng-Han}
\email{emiliaistruelove@gmail.com}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\author{Chun-Wei Tsai}
\email{cwtsai@mail.cse.nsysu.edu.tw}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\begin{abstract}

    % dont we
    % To address
    % To evaluate
    Although the training-free NASs are typically faster than training-based 
    NAS method, however, the correlation between score value and the result 
    of an architecture is not well enough in most cases.
    To address this problem, we propose a genetic-based training-free NAS 
    algorithm with hybrid training-free score function, which combines three 
    highly heterogeneous training-free score functions to evaluate an architecture. 
    In this method, the genetic algorithm plays a role to guide the searches 
    of NAS algorithm while the hybrid training-free score function plays the 
    role to evaluate a new candidate architecture during the convergencr process 
    of GA. More precisely, the first score function is noise immunity for 
    neural architecture search without search (NINASWOT), as an evaluation of 
    pattern recognition ability, second one is condition number of neural 
    tangent kernel (NTK), as an evaluation of trainability.
    
    To evaluate the performance of the proposed algorithm, this paper compared 
    it with several NAS algorithms, including weight-sharing methods, 
    non-weight-sharing methods, and neural architecture search without training (NASWOT). 
    We expect develope a faster and more accurate training-free NAS algorithm.

\end{abstract}
\maketitle

\section{Introduction}
\label{sec:introduction}

% introduction
    Neural architecture search (NAS) has recently drawn a big amount of 
    attention, since the ability to automatically design a "good" neural 
    architecture. By leveraging machine learning algorithms \cite{https://doi.org/10.48550/arxiv.1611.01578}, 
    NAS algorithms can explore a search space, which is comprised of numerous 
    potential architectures, to find out a good architectures that outperform 
    those designed by human experts. "In recent years", the use of NAS is 
    widespread, from object detection \cite{https://doi.org/10.48550/arxiv.2111.13336}, 
    image recognition \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    and speech recognition \cite{https://doi.org/10.48550/arxiv.2011.05649} 
    \cite{mehrotra2021nasbenchasr} to natural language 
    processing. \cite{jiang-etal-2019-improved} 
    \cite{https://doi.org/10.48550/arxiv.2006.07116} 
    \cite{https://doi.org/10.48550/arxiv.2005.14187}

    % reduce 
    Despite the promising results of NAS, there are still many challenges 
    to conquer. One major problem is the extremely high computational 
    cost to search for an optimal architecture, which can make NAS impractical 
    for real-world applications, particularly on resource-constrained 
    platforms like embedded system. The reason why NAS is costly is that 
    during the searching, a candidate architecture must be trained to 
    evaluate how good of this architecture.

    To overcome this challenge, recent works developed and proposed lots 
    of method which is so called training-free NAS. 
    % NINASWOT
    For example, Mellor et al. \cite{https://doi.org/10.48550/arxiv.2006.04647} proposed the 
    measurement of the correlation between the binary activation paterns, 
    induced by the untrained network at two inputs, named as neural architecture 
    search without training (NASWOT). 
    Later, Wu et al. \cite{10.1145/3491396.3506510} found a high score 
    obtained by such a function may not correspond to a high-performence 
    model. Thus, they additionally applied noisy immunity method on NASWOT, 
    named as noisy immunity for NASWOT (NINASWOT).
    % NTK
    For another example, Chen et al. proposed to compute the condition 
    number of neural tangent kernel (NTK) \cite{https://doi.org/10.48550/arxiv.2102.11535} 
    \cite{https://doi.org/10.48550/arxiv.2203.09137} \cite{https://doi.org/10.48550/arxiv.2109.00817}, 
    which is used as the score to estimate the trainability of an architecture.
	% unknown

% what is unknown?
    However, most of score functions suffer from low correlation between 
    score value and the result of an architecture, leading to a predicament 
    that no matter how good the search method is used, we can hardly find 
    an optimal architecture. 
    The major problem causes the low correlation is that a single score 
    function can only evaluate one perspect/characteristic of an architecture.
    
% method
    To address the problem, we propose cooperating three heterogeneous score 
    functions with genetic-based search method, which evaulate an architecture 
    from different aspects. More specifically, the first score function is 
    NINASWOT \cite{10.1145/3491396.3506510} as an indicator of the ability 
    to distinguish two images. The second one is the condition number of NTK 
    \cite{https://doi.org/10.48550/arxiv.2102.11535} as score to estimate the 
    trainability of an architecture. The last one is (TBD).

% conclusion
    The major contribution of this paper can be summarized as follow:
    \begin{itemize}
        \item Develope a genetic-based neural architecture search method based on three hybrid score functions.
        \item Cooperating three score functions to get a higher correlation between score and accuracy.
    \end{itemize}
    % expect what

    The remainder of this paper is organized as follows: Section 2 provides the 
    detail about the three heterogeneous score functions. Section 3 gives a detailed 
    description about the proposed method. Section 4 provide the simulation results 
    in different search space. The conclusion and further prospect are gived in 
    Section 5.

    \section{Related Works}
    \begin{figure*}[htb]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/naswot.pdf}}
        \caption{A simple example to illustrate the procedure of NASWOT.}
        \label{fig:naswot}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}
    In these days, the studies of training-free neural architecture search (NAS) 
    go viral, since the ability to accelerate the procedure for designing a neural 
    network architecture used on specific application, while additionally, 
    training-free NAS cleverly avoid the drawback of long-time training by using 
    various training-free score functions. 

    % naswot
    In \cite{https://doi.org/10.48550/arxiv.2006.04647}, Mellor et al. proposed a 
    score function without the requirement for training. Here we call it neural 
    architecture search without training (NASWOT) score function. \xfig{fig:naswot} 
	gives a simple example to illustrate the procedure of NASWOT score function. 
    Consider a mini-batch of data $X=\{x_i\}^N_{i=1}$ passing through a neural network 
    architecture as $f(x_i)$. The activated ReLU units in every layer of the architecture 
    form a binary code $c_i$ that define the linear region.
    The correlation between binary codes for the whole mini-batch can be examined 
    by computing the kernel matrix 
    \begin{equation}
        K_H=\begin{pmatrix}N_A-d_H(c_1,c_2)&\cdots&N_A-d_H(c_1,c_N)\\\vdots&\ddots&\vdots\\N_A-d_H(c_N,c_1)&\cdots&N_A-d_H(c_N,c_1)\end{pmatrix}
    \end{equation}
    where $N_A$ is the number of ReLU units and $d_H(c_i,c_j)$ is the hamming 
    distance between the binary code $c_i$ and $c_j$. 
    With the kernel matrix, the score of an architecture can be evaluate as 
    follow: 
    \begin{equation}
        s=\log\lvert K_H\rvert
    \end{equation}
    The rationale behind is to see how dissimilar the linear region activated by 
    the ReLU units between two inputs. An architecture shall learn better 
    when inputs are well separated.
	
    % ninaswot
    \begin{figure*}[htb]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/ni.pdf}}
        \caption{A simple example to illustrate the procedure of noise immunity.}
        \label{fig:ni}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}
    Based on the work of Mellor et al., Wu et al. \cite{10.1145/3491396.3506510} 
    found, in some case, an architecture with high score may put different 
    input data, which are originally in the same class, to different classes. 
    To fix this defect, Wu et al. proposed using noise immunity to additionally 
    evaluate an architecture. \xfig{fig:ni} gives an example to illustrate how 
    noise immunity score evaluate an architecture. The score function picks a 
    mini-batch of data, denoted $X$, and then applies Gaussion noise on it, 
    denoted $X'$ and defined by $X'=X+z$ where $z$ is the Gaussion noise. 
    By passing $X$ and $X'$ through the untrained architecture, the sum of the 
    square differences between outputs 
    $O=\begin{matrix}o_1,o_2,\cdots,o_C\end{matrix}$ and 
    $O'=\begin{matrix}o_1',o_2',\cdots,o_C'\end{matrix}$ can be calculated as 
    follows: 
    \begin{equation}
        n=-\sum^C_{n=1}(o_i-o_i')^2
    \end{equation}
    where $C$ is the number of classes determined by the input data.
    According to the experiment outcome of Wu et al., noise immunity can separate 
    those are unable to distinguish $X$ and $X'$ from architectures with high NASWOT 
    score, which just complements the NASWOT score funcion.
    
    % ntk
    On the other hand, instead of using the correlation of binary activation 
    patterns as score. Chen et al. proposed using the condition number of NTK 
    as an evaluation of the trainability, defined as % add figure
    \begin{equation}
        \mathcal{K_N}=\frac{\lambda_{\textrm{max}}(\hat\Theta)}{\lambda_{\textrm{min}}(\hat\Theta)}
    \end{equation}
    where $\lambda_{\textrm{max}}(\hat\Theta)$ and $\lambda_{\textrm{min}}(\hat\Theta)$ 
    are the maximum and minimum eigenvalues of NTK ($\hat\Theta$) respectively.
    The rationale is based on the work of Lee et al. \cite{Lee_2020} and 
    Xiao et al. \cite{https://doi.org/10.48550/arxiv.1912.13053}, which concluded briefly here, 
    the training dynamics of a wide neural network can be written in terms of the 
    spectrum of the NTK:
    \begin{equation}\label{eq:6}
        \mu_t(\textbf{X}_{\textrm{train}})_i=(\textbf{I}-e^{-\eta\lambda_it})\textbf{Y}_{\textrm{train,}i}
    \end{equation}
    where $\lambda_i$ are the eigenvalues of $\hat\Theta_{\textrm{train}}$ 
    and they're ordered as $\lambda_0\geq\cdots\geq\lambda_m$. 
    According to the hypothesis proposed by Lee et al. \cite{Lee_2020}, the maximum 
    feasible learning rate scale as $\eta\sim2/\lambda_0$. Plug this scaling for 
    $\eta$ in Eq. \ref{eq:6}, we see that the $\lambda_m$ will converge exponentially 
    at a rate given by $1/K_N$ where $K_N=\frac{\lambda_0}{\lambda_m}$.
    Then it can be concluded that if $K_N$ is lower, the corresponding architecture is 
    more trainable.

	% synflow?
	
    % conclusion

    \section{the proposed algorithm}
    In our work, however, we use rank-based GA. Rather than use single fitness to select, 
    the method preserve the top 1 in NINASWOT score and also the top 1 in the condition 
    number of NTK. After the evolution, 

    \bibliographystyle{IEEEtran}
    \bibliography{main}
\end{document}

