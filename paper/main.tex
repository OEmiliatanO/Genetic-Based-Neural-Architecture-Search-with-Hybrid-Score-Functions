%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass[sigconf]{acmart}
\let\Bbbk\relax %% fix bug
\usepackage[utf8]{inputenc}

% =======================
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsopn}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{textcomp}

\usepackage{boxedminipage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{url}
\usepackage{times}
\usepackage{version}
% \usepackage[pdftex]{graphicx}
\usepackage{epsfig}
\usepackage{epsf}
%\usepackage{graphics}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\PassOptionsToPackage{bookmarks={false}}{hyperref}
%%%%%%%%%%%%
\usepackage{comment}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{dblfloatfix}
% ==========================
\input lstset.tex
\input macros.tex

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\algrenewcommand\alglinenumber[1]{\footnotesize #1}

\algnewcommand\algorithmicassume{\textbf{Assumption:}}
\algnewcommand\Assume{\item[\algorithmicassume]}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}

\algnewcommand\algorithmicparameter{\textbf{Parameter:}}
\algnewcommand\Parameter{\item[\algorithmicparameter]}


\begin{document}

\title{Rank-based Training-Free NAS Algorithm}

\author{Hsieh Cheng-Han}
\email{emiliaistruelove@gmail.com}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\author{Chun-Wei Tsai}
\email{cwtsai@mail.cse.nsysu.edu.tw}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\begin{abstract}
    %Although the training-free neural architecture search (NAS) are typically 
    %faster than training-based NAS methods, however, the correlation between 
    %the measurement and the final accuracy of an architecture is not well 
    %enough in most cases. 
    The training-free neural architecture search (NAS) typically uses one or 
    two score functions to evaluate how an neural network architecture performs. 
    However, an architecture shall be treated as a complex system who has 
    distinct characteristics to measure. 
    To achieve the goal, we propose a rank-based training-free NAS algorithm, 
    which combines three training-free score functions which is "complementary" 
    to each others to evaluate an architecture by ranking. 
    In this work, noise immunity (NI) considers the image generalization 
    ability of the architecture, the correlation between binary activation 
    paterns, named as NASWOT, considers image distinction ability, and LogSynFlow 
    takes trainability into account. With that, a modified version of simulated 
    annealing (SA) algorithm, \palg, can applies on and obtains a better 
    performance on searching high accuracy architectures. 
    To evaluate the performance of the proposed algorithm, this paper compared 
    it with several NAS algorithms, including weight-sharing methods, 
    non-weight-sharing methods, and several state-of-the-art training-free 
    score functions. The final result indicates that \palg outperforms most 
    of training-free score functions and shows the robustness between different 
    search spaces. 

\end{abstract}
\maketitle

\section{Introduction}
\label{sec:introduction}

% introduction
    Neural architecture search (NAS) has recently drawn a big amount of 
    attention, since the ability to automatically design a "good" neural 
    architecture. By leveraging machine learning algorithms 
    \cite{https://doi.org/10.48550/arxiv.1611.01578}, NAS algorithms can 
    explore a search space, which is comprised of numerous potential 
    architectures, to find out a good architectures that outperform those 
    designed by human experts. Recently, the use of NAS is widespread, 
    from object detection \cite{https://doi.org/10.48550/arxiv.2111.13336}, 
    image recognition \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    and speech recognition \cite{https://doi.org/10.48550/arxiv.2011.05649} 
    \cite{mehrotra2021nasbenchasr} to natural language processing (NLP). 
    \cite{jiang-etal-2019-improved} \cite{https://doi.org/10.48550/arxiv.2006.07116} 
    \cite{https://doi.org/10.48550/arxiv.2005.14187} 
    Despite the promising results of NAS, there are still many challenges 
    to conquer. One major problem is the extremely high computational 
    cost to search for an optimal architecture, which can make NAS 
    impractical for real-world applications, particularly on resource-constrained 
    platforms like embedded system. The reason why NAS is costly is that 
    during the searching, a candidate architecture must be trained to 
    evaluate how good of this architecture. 

    To overcome this challenge, recent works developed and proposed lots 
    of method which is so called training-free NAS. 
    % NASWOT
    For example, Mellor et al. \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    proposed the measurement of the correlation between the binary 
    activation paterns, induced by the untrained network at two inputs, 
    abbreviated as NASWOT. 
    % synflow
    On the other hand, Lee et al. \cite{lee2019snip} proposed purning 
    parameters based on a saliency matric, which then extended by Tanaka 
    et al. \cite{tanaka2020pruning}. In \cite{tanaka2020pruning}, Tanaka 
    et al. proposed Iterative Synaptic Flow Pruning (SynFlow) algorithm 
    which intends to deal with the layer collapse problem when purning a 
    network. The score function used in the algorithm is so-called 
    \it{synaptic saliency} \rm{score}. Later, Abdelfattah et al. 
    \cite{abdelfattah2021zerocost} extended SynFlow to score a network 
    architecture by summing synaptic saliency score over all parameters 
    in the model. Cavagnero et al. \cite{Cavagnero_2023} found that 
    SynFlow is likely to suffer from gradient explosion, then proposed 
    the LogSynFlow score function which prevents the problem. 
    % NTK
    For another example, Chen et al. 
    \cite{https://doi.org/10.48550/arxiv.2102.11535} proposed to compute 
    the condition number of neural tangent kernel (NTK) 
    \cite{https://doi.org/10.48550/arxiv.2203.09137} 
    \cite{https://doi.org/10.48550/arxiv.2109.00817}, which is used as 
    the score to estimate the trainability of an architecture. 

% what is unknown?
    % table放第一頁 不然就不要放
    % 做成圖表放第一頁
    However, \xtab{table:corr_score} shows most of score functions suffer 
    from low correlation between score values and the final accuracy of 
    architectures, leading to a predicament that no matter how good the 
    search method is used, we can hardly find an optimal architecture. 
    The major problem causes the low correlation is that a single score 
    function can only evaluate one characteristic of an architecture. 
    \begin{table}[tb]
        \caption{\textsc{The Kendall correlation between training-free score and test accuracy, evaluated on the three datasets of NATS-Bench \cite{Dong_2021}. }}
        \resizebox{0.48\textwidth}{!}{
            \begin{tabular}{cccc}\toprule
                Training-free score function & CIFAR-10 & CIFAR-100 & ImageNet-16-120 \\ 
                \midrule
                \bf{NTK}                     & -0.33    & -0.30     & -0.39 \\
                \bf{Snip}                    & 0.45     & 0.47      & 0.41  \\
                \bf{Fisher}                  & 0.39     & 0.40      & 0.36  \\
                \bf{Grasp}                   & 0.28     & 0.35      & 0.35  \\
                \bf{NASWOT}                  & 0.61     & 0.62      & 0.60  \\
                \bf{SynFlow}                 & 0.57     & 0.56      & 0.56  \\
                \bf{LogSynFlow}              & 0.61     & 0.60      & 0.59  \\
                \bf{Rank (ours)}             & X        & X         & X     \\ \bottomrule
                \label{table:corr_score}
            \vspace{-\baselineskip}
            \end{tabular}
        }
    \end{table}
    \begin{figure*}[b]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/naswot.pdf}}
        \caption{A simple example to illustrate the procedure of NASWOT.}
        \label{fig:naswot}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}
% method
    To address the problem, we propose cooperating three complementary 
    score functions by ranking, which allows every score functions 
    evaluate an architecture without losing fairness. 
    The first score function is noise immunity (NI) \cite{10092788}, as 
    a measurement of the ability of image generalization. 
    The second one is NASWOT, as score to estimate the ability of distinction 
    of an architecture and also a complement of NI. 
    The last one is LogSynFlow used as the measurement of trainability 
    and also as a complement of NI and NASWOT. 
    With these three complementary score functions, an evaluation of an 
    architecture is not simply from a single aspect but three different 
    aspects, which is like estimate the weight of an object by not only 
    the length but its height and width. 
    %compares the different search algorithms for NAS, e.g., random search, genetic 
    %algorithm (GA) \cite{mitchell1998introduction}, simulated annealing algorithm (SA)
    %\cite{kirkpatrick1983optimization}. 

    % conclusion
    %The major contribution of this paper can be summarized as follow:
    %\begin{itemize}
    %    \item Cooperating three score functions to obtain better ability for searching top accuracy neural architectures. 
    %    \item Develope simulated annealing algorithms with rank-based score. 
    %\end{itemize}
    % expect what

    The remainder of this paper is organized as follows: 
    \xsec{sec:related_work} provides the detail about the three 
    complementary score functions. \xsec{sec:proposed} gives a detailed 
    description about the proposed method. \xsec{sec:results} begins 
    with parameters setting and provide the simulation results following 
    is the experiment results in different search space. The conclusion 
    and further prospect are gived in \xsec{sec:conclusion}.

    \section{Related Works}
    \label{sec:related_work}
    \subsection{Training-free Score Functions}
    In \cite{https://doi.org/10.48550/arxiv.2006.04647}, Mellor et al. 
    proposed a score function without the requirement for training which 
    is abbreviated as NASWOT. \xfig{fig:naswot} gives a simple example 
    to illustrate the procedure of NASWOT score function. Consider a 
    mini-batch of data $X=\{x_i\}^N_{i=1}$ passing through a neural network 
    architecture. The activated ReLU units in every layer of the architecture 
    form a binary code $c_i$ that define the linear region. The correlation 
    between binary codes for the whole mini-batch can be examined by computing 
    the kernel matrix 
    \begin{equation}
        K_H=\begin{pmatrix}N_A-d_H(c_1,c_1)&\cdots&N_A-d_H(c_1,c_N)\\\vdots&\ddots&\vdots\\N_A-d_H(c_N,c_1)&\cdots&N_A-d_H(c_N,c_N)\end{pmatrix},
    \end{equation}
    where $N_A$ is the number of ReLU units and $d_H(c_i,c_j)$ is the hamming 
    distance between the binary code $c_i$ and $c_j$. 
    With the kernel matrix, the score of an architecture can be evaluated as 
    follow: 
    \begin{equation}
        s=\log\lvert K_H\rvert,
    \end{equation}
    The rationale behind is that, the more different between the binary codes 
    the better the architecture learns. And the determinant of the kernel matrix 
    measures how "different" they are by calculating the volume formed by the 
    row vector of $K_H$. 

    Wu et al. \cite{10092788} found, in some case, an architecture with high 
    NASWOT score may classify the same kind of input data into different classes. 
    To fix this defect, Wu et al. proposed using noise immunity (NI) to 
    evaluate an architecture. \xfig{fig:ni} gives an example to illustrate how 
    NI evaluate an architecture. The score function picks a 
    mini-batch of data, denoted $X$, and then applies Gaussion noise on it. 
    The process can be defined by $X'=X+z$ where $z$ is the Gaussion noise. 
    By passing $X$ and $X'$ through the untrained architecture, then computing 
    the difference of square of each feature maps captured at all pooling layers. 
    Calculate the matrix $\kappa$ defined by 
    \begin{equation}
        \label{equ:ni_kappa}
        \kappa=\begin{pmatrix}\frac{(\tau_{1,1}-\tau'_{1,1})^2}{\lvert \tau_{1,1}\rvert}&\cdots&\frac{(\tau_{1,N}-\tau'_{1,N})^2}{\lvert \tau_{1,N}\rvert}\\\vdots&\ddots&\vdots\\\frac{(\tau_{L,1}-\tau'_{L,1})^2}{\lvert \tau_{L,1}\rvert}&\cdots&\frac{(\tau_{L,N}-\tau'_{L,N})^2}{\lvert \tau_{L,N}\rvert}\end{pmatrix},
    \end{equation}
    where $L$ is the total number of pooling layers; $N$ the size of mini-batch, 
    and $\tau_{i,j}$ and $\tau'_{i,j}$ are the feature maps which $X$ passing and 
    the one perturbed by $X'$ at the $i$-th pooling layer and $j$-th input 
    data of the mini-batch, respectively. 
    Then $\eta$ can be calculated by 
    \begin{equation}
        \label{equ:ni_eta}
        \eta=\ln(\epsilon+e_L\kappa e_N),
    \end{equation}
    where $\epsilon$ is a small positive number, and $e_L$ and $e_N$ are a row 
    vector of 1's of size $L$ and a column vector of 1's of size $N$, respectively. 
    According to the fact that the input data are the same kind, the smaller $\eta$ 
    is, the better the noise immunity of the architecture is.

    \begin{figure*}[tb]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/ni.pdf}}
        \caption{A simple example to illustrate the procedure of NI.}
        \label{fig:ni}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}

    % synflow
    Besides from image-related aspects, there is another aspect to evaluate a network. 
    \textit{The Lottery Ticket Hypothesis} \cite{frankle2019lottery}, reveals that a 
    network may have a "core" which decides the final accuracy of the network. 
    How to pruning a network correctly is therefore tempting. Lee et al. 
    \cite{lee2019snip} proposed to use connection sensitivity as a criterion to prune 
    the network which can be briefly viewed as 
    \begin{equation}
        \label{equ:snip_connection_sensitivity}
        s_j=\lvert \frac{\partial \mathcal L}{\partial w_j}w_j\rvert,
    \end{equation}
    where $w_j$ the $j$-th element of the weight of the netwrok, and $\mathcal L$ is 
    the empirical risk. The meaning behind is to approximate the contribution 
    to the change of the loss function from a specific connection. By pruning the 
    connections which has relatively small contribution, the expensive prune, retrain 
    cycles, can be prevented. 
    Later, Wang et al. \cite{wang2020picking} noticed that SNIP with a high pruning ratio 
    tends to cause \textit{layer-collapse}, which prunes all parameters in a single weight 
    layer. Therefore, they propose Gradient Signal Preservation (GraSP) algorithm, which 
    aims to preserve the gradient flow at initialization by approximating the change in 
    gradient norm defined as 
    \begin{equation}
        \label{equ:grap}
        S_p(\theta)=-(\textbf{H}\frac{\partial \mathcal L}{\partial \theta})\odot\theta,
    \end{equation}
    where $\textbf{H}$ is the Hessian matrix, $\theta$ the parameters, and $\odot$ is Hadamard product. 
    In \cite{tanaka2020pruning}, to solve the problem that the existing pruning algorithms, e.g., 
    Magnitude, SNIP, GraSP, using global-masking usually encounter layer-collapse which will make 
    the pruned network untrainable. 
    Tanaka et al. generalized the synaptic saliency scores as 
    \begin{equation}
        \label{equ:synflow}
        S_p(\theta)=\frac{\partial \mathcal R}{\partial \theta}\odot\theta,
    \end{equation}
    where $\mathcal R$ is a scalar loss function, and proposed Iterative Synaptic Flow Pruning (SynFlow) 
    algorithm which is an extension of magnitude pruning algorithm and avoids layer-collapse. 
    Abdelfattah et al. \cite{abdelfattah2021zerocost} extended the work, proposed to score a 
    network by summing the synaptic saliency score over all parameters in the model, which is 
    defined as 
    \begin{equation}
        \label{equ:zero_cost}
        S_n=\sum^N_i S_p(\theta)_i.
    \end{equation}
    The rationale behind is to calculate all the contribution to the loss function of parameters. 
    The higher the score of an architecture, the more trainable this architecture is. 
    Finally, Cavagnero et al. \cite{Cavagnero_2023} imporved SynFlow, which is likely to fall into 
    gradient explosion problem, and proposed LogSynFlow which simply scaling down the gradient. 
    The formula is defined by 
    \begin{equation}
        \label{equ:logsynflow}
        S(\theta)=\theta\cdot\log(\frac{\partial \mathcal L}{\partial \theta}+1)
    \end{equation}

    Except using the synaptic saliency score to measure the trainability, 
    \cite{https://doi.org/10.48550/arxiv.2102.11535} uses the condition number of neural tangent 
    kernel (NTK) \cite{jacot2020neural} to measure it, which the score can be defined as 
    \begin{equation}
        \label{equ:cn_ntk}
        \frac{\lambda_m}{\lambda_0}, 
    \end{equation}
    where $\lambda_0$ is the biggest eigenvalue and $\lambda_m$ is the smallest eigenvalue of the 
    NTK. The rationale behind is the converge rate of the network is condition by the rate 
    $\frac{\lambda_m}{\lambda_0}$. The higher the condition is, the faster the converge. 

    \subsection{Search Algorithms}
    First assume that input data $D$ are separated into two subset, $D_r$ and $D_t$. Furthermore, 
    assume $\mathcal F_A$ is the accuracy function, $\mathcal F_S$ the score function, $\mathcal A_s$ the 
    search algorithm of NAS, and $\mathcal A_L$ the learning algorithm. Then, NAS problem can be 
    described as an optimization problem as follows: 
    \begin{equation}
        \label{equ:nas}
        \mathbb N^*=\max_{\mathbb N^b\in\mathbb N}\mathcal F_A(\mathcal A_L(\mathbb N^b, D_r), D_t)
    \end{equation}
    where $\mathbb N$ is a set of neural architectures, namely, the search space of NAS. 
    For non-training-free NAS 
    \begin{equation}
        \label{equ:non-training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_A(\mathcal A_L(\mathbb N^s, D_r), D_t), \mathbb N)
    \end{equation}
    and for training-free NAS 
    \begin{equation}
        \label{equ:training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_S(\mathbb N^s, D_r), \mathbb N)
    \end{equation}
    It can be seen that training-free NAS is comprised of two part, search algorithm $\mathcal A_S$ and score 
    function $\mathcal F_S$. 
    There are several search algorithms applied on NAS, including random search, reinforcement learning, and
    metaheuristic algorithm. 
    
    In \cite{https://doi.org/10.48550/arxiv.2006.04647} \cite{Lopes_2021}, random search is applied on. 
    The advantage of random search is simple, easy to implement, and the result can be token as a 
    baseline compared to more comprehensive search algorithm. Generally speaking, when applying random 
    search on a samll search space, e.g., nasbench201 \cite{dong2020nasbench201}, the performance is 
    similar to other search algorithms. But when it comes to a relatively larger search space, e.g., nasbench101 
    \cite{ying2019nasbench101} and natsbenchsss \cite{Dong_2021}, random search can no longer standout 
    in other search algorithms. 

    In \cite{zoph2017neural}, reinforcement learning is used to find the maximum accuracy of an architecture 
    generated by a network architecture controller. The actions $a_{1:T}$ of the reinforcement learning is 
    equivalent to updating the parameters $\theta_c$ for the controller. The goal is to maximize 
    its expected reward, defined by 
    \begin{equation}
        \label{equ:reinforcement_rw}
        J(\theta_c)=E_{P(a_{1:T};\theta_c)}[R]
    \end{equation}
    and the gradient of $J(\theta_c)$ is defined by
    \begin{equation}
        \label{equ:reinforcement_grad}
        \nabla_{\theta_c} J(\theta_c)=\sum^T_t=1 E_{P(a_{1:T};\theta_c)}[\nabla_{\theta_c}\log P(a_t\mid a_{(t-1):1};\theta_c)R]
    \end{equation}

    As for an example of metaheuristic algorithm, in \cite{10.1145/3491396.3506510}, Wu et al. leveraged 
    genetic algorithm (GA) as search strategy. 
    By encoding the network architecture, the architecture can be viewed as gene. Therefore, GA can be 
    easily applied on. 
    Later in \cite{10092788}, Wu et al. used search economic (SE) \cite{7379579} as search strategy. The 
    basic idea of SEs is to first divide the search space into a set of subspaces and investiagte those 
    subspaces based on the expected value of each subspace. The expected value is comprised of 
    \begin{itemize}
        \item The number of times the subspace has been investiagted. 
        \item The average objective value of the new candidate solutions in the subspace at current iteration. 
        \item The best solution so far in this subspace. 
    \end{itemize}
    Based on these design, SE can avoid fall into local optimum, while search for high expected value instead. 
    
    \section{Methods}
    \label{sec:proposed}

    The motivation origins from these three questions: 

    \textit{1. Can we combine multiple score functions to improve the search performance 
    or the correlation between score values and the final accuracy of architectures?}
    
    The answer is yes. If the chosen score functions is complementary to 
    each others, the new correlation can eventually beyond the original ones. \cite{10.1145/3491396.3506510} 
    shows that possibility. When an architecture gain a high NASWOT score, it may classify 
    images, which are in the same class originally, into different classes. NI here comes 
    to rescue. By measuring the noise immunity of an architecture, the miss-classifying problem 
    can be solved. Thus, if mix NASWOT with NI, the correlation and performance increases. 
    An example is shown in \xfig{fig:ninaswot}.
    \begin{figure*}[htb]
        \vspace{-\baselineskip}
        \begin{center}
            \begin{tabular}{ccc}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/naswot-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ni-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ninaswot-acc.pdf}}}
            \end{tabular}
            \caption{(a) NASWOT score for 1,000 randomly chosen architectures from NAS-Bench-201 in the CIFAR-10 dataset 
            (b) NI score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset. 
            (c) NI score + NASWOT score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset.}
            \label{fig:ninaswot}
        \end{center}
        \vspace{-\baselineskip}
    \end{figure*}

    \textit{2. How to choose the score functions?}

    As mentioned, the key is to choose the complement of a score function. For example, 
    score functions that evaluate the expressivity/distinction ability shall have other 
    score functions to evalute the generalization ability. For these two graphic recognition 
    score functions, the complement of them can be a measurement of trainability. 

    \textit{3. How to combine multiple different kinds of score functions together 
    and prevent one from overwhelming the others numerically?}
    
    One possible way is normalization, but the necessary information is known only 
    after evaluating the whole search space, e.g., range, mean or standard deviation, 
    which is practically impossible. In order to make the proposed score function 
    agnostic to the search space, it leverages multiple score functions by ranking, 
    which also provides a solution to have equal contribution from each score function. 

    \subsection{Simulated Annealing with Ranking Algorithm}

    The modified version of the simulated annealing (SA) algorithm with ranking algorithm (\palg)
    is outlined in \xalg{alg:SA}. The first step is to sample an architecture as current 
    solution, denoted $s$, from search space. Then use $s$ as a seed to generate the 
    neighbourhood of it, denoted $U$, and the size of it is decided by a presetting parameter, 
    the number of generated neighbours, $N$. 
    After ranking with the seed, the neighbourhood and the historical best architectures set, $H$, 
    it is maintained by adding the architecture (including $s$) which has highest rank, and 
    the architecture (excluding $s$) which has highest rank is denoted by $s^*$. 
    If the rank of $s^*$ is higher than $s$, the current solution will be 
    replaced by $s^*$, or be replaced by probability, defined by 
    \begin{equation}
        \label{equ:SA_rk_prob}
        e^{-\alpha\frac{\Delta E}{T}},
    \end{equation}
    where $T$ is the current temperature, $\Delta E$ the difference of the rank between 
    $s$ and $s^*$, and $\alpha$ is a constant number adjusts the probability. The bigger 
    the difference, the smaller the probability to accept the new solution. 
    After the $I$ iterations, the temperature scales down by $r_t$. The algorithm 
    is done when $T\leq\tau$, and the current architecture will be retruned. 
    Notbly, the size of $H$ is based on the maximum searching number of SA, defined by 
    \begin{equation}
        \label{equ:SA_rk_size_of_H}
        \beta (I\cdot N\cdot\frac{\ln{\tau/T}}{\ln{r_t}}),
    \end{equation}
    where $\beta$ is a constant number between 0 and 1.

    \begin{algorithm}[h]
        \caption{The Simulated Annealing with Ranking Algorithm}\label{alg:SA}
        \begin{algorithmic}[1]
            \Input{search space $\mathbb N^*$}
            \Parameter Initial temperature $T$, ending temperature $\tau$, a real number between 0 and 1, $r_t$, the number of iteration, $I$, the number of generated neighbours, $N$, a real number, $\alpha$
            \State Initialize historical best architectures set, $H$
            \State $s=\text{Randomly sample a architecture from }\Bbb N^*$
            \While{$T>\tau$}
                \For{$\text{ each }i \in [1\ldots I]$}
                    \State $U = \text{neighbour(s, }N\text{)}$
                    \State $\text{Ranking}(\{s\}\cup U\cup H)$
                    \State Maintain historical best architectures set, $H$
                    \State $s'=\text{Best architecture excluding }s$
                    \State $\Delta E=\text{The difference of rank between }s'\text{ and }s$
                    \State Sample $x$ from uniform distribution between 0 and 1
                    \If{$\Delta E<0$}
                        \State $s\leftarrow s'$
                    \Else
                        \If{$x\leq e^{-\alpha\frac{\Delta E}{T}}$}
                            \State $s\leftarrow s'$
                        \EndIf
                    \EndIf
                \EndFor
                \State $T\leftarrow T\times r_t$
            \EndWhile
            \State $\textbf{Return }s$ \Comment{Return the current architecture}
        \end{algorithmic}
    \end{algorithm}

    \subsection{Encoding Schema}

    The encoding schema determines the efficiency of searching. The naïve encoding 
    schema is to use a binary/integer vector as a representation of an architecture. 
    For example, NAS-Bench-201 provides 5 operations and 6 nodes to select an cell 
    and cells form an architecture, which implies the whole search space can be 
    encoded into a 6 integers vector. However, a broken architecture appears since 
    one of the operations is zeroize, which means the measurement of the broken 
    architecture is meaningless. To exclude the broken architectures, the encoding 
    schema in \cite{10092788} is used in this paper. The detail of the encoding 
    schema differs from each search spaces, but they all share the common core idea, 
    preserving the critical path in an architecture. The encoding schema guarantees 
    that there exists at least one path from input to output in a cell, and no 
    zeroize operation is in it. Based on the encoding schema, the efficiency of 
    searching can be improved, since it excludes the broken architectures. 

    \subsection{Ranking Algorithm}

    The general proposed ranking algorithm is outlined in \xalg{alg:rank-based}. 
    The first step is to evaluate the network architectures in the subset, 
    denoted $\mathbb N^s$, of search space, denoted $\mathbb N^*$. The 
    evaluation of an architecture determined by $j$-th score function is 
    denoted $s_{\mathcal F_j}(\mathbb N^i)$. After evaluating the subset 
    of search space, the rank of each score function is calculated, denoted 
    $r_{\mathcal F}$. Then the final rank, $r$, calculated by summing up 
    the ranks. If a network architecture gains higher score in these score 
    functions, the final rank should be higher (smaller index) too. Therefore, 
    the highest (minimum) rank in $r$ shall be the best network architecture. 
    In summary, the rank-based score function makes the contribution of 
    each score function even, and therefore evaluate an architecture from 
    different aspects. 

    \begin{algorithm}[h]
        \caption{The Ranking Algorithm}\label{alg:rank-based}
        \begin{algorithmic}[1]
            \Input{A subspace, ${\mathbb N}^{s}$, from search space, ${\mathbb N}^{*}$}
            \For{$\text{each }{\mathbb N}^i\text{ in }{\mathbb N}^s$}
                \For{$\text{each }j\in\{1,\ldots,M\}$}
                    \State $s_{{\mathcal F}_{j}}({\mathbb N}^i)={\mathcal F}_{j}({\mathbb N}^i)$
                \EndFor
            \EndFor
            \For{$\text{each }j\in\{1,\ldots,M\}$}
                \State $r_{{\mathcal F}_{j}}({\mathbb N}^i)$ = index of ${\mathbb N}^i$ in list $[s_{{\mathcal F}_{j}}({\mathbb N}^1),\ldots,s_{{\mathcal F}_{j}}({\mathbb N}^{\lvert {\mathbb N}^s\rvert})]$ \par sorted by descending order
            \EndFor
            \State $r({\mathbb N}^i)=\sum^M_{j=0} r_{{\mathcal F}_{j}}({\mathbb N}^i)$
            %\State $i^*=\arg\min_{i}\{r({\mathbb N}^i)\}$
            %\State $\textbf{Return }{\mathbb N}^{i^*}$
            \State $\textbf{Return }r$
        \end{algorithmic}
    \end{algorithm}

    With the proposed ranking algorithm as framework, the next question is 
    to choose score functions that are complementary. The recent training-free 
    score functions can be roughly classified into different kinds. NASWOT, 
    linear regions distribution \cite{https://doi.org/10.48550/arxiv.2102.11535} \cite{lin2021zennas}, 
    maximum entropy of an architecture \cite{sun2022maedet} are measurements 
    to the ability of distinction and expressivity of an architecture; 
    NI is a measurement to the ability of image generalization of an architecture; 
    the condition number of NTK, modified version of SNIP, GraSP, SynFlow, and 
    LogSynFlow are measurements to the ability of trainability of an architecture. 
    As mentioned, the chosen score function shall be complementary to each other. 
    In this paper, NI and NASWOT are used as a complementary set, which take 
    the ability of images generalization and distinction into account. 
    These complementary set can be generalized as the ability of the graphic 
    recognition. Therefore, the last missing piece is trainability of an 
    architecture, which, LogSynFlow is chosen to measure it. 

    \section{Experiment Result}
    \label{sec:results}

    We evalute the performance of the proposed algorithm, \palg, 
    for searching the architectures of high accuracy. For comparison, several NAS 
    algorithms are included, e.g., random search (RS), regularized evolution algorithm 
    (REA) \cite{real2019regularized}, reinforcement learning (RL) \cite{Williams:92}, 
    Bayesian optimization with hyper band (BOHB) \cite{falkner2018bohb}, RS with 
    weight-sharing (RSWS) \cite{li2019random}, differentiable architecture search 
    with first order (DARTS-V1) \cite{liu2019darts}, DARTS with second order 
    (DARTS-V2) \cite{liu2019darts}, gradient-based search using differentiable 
    architecture sampler (GDAS) \cite{dong2019searching}, self-evaluated template 
    network (SETN) \cite{Dong_2019}, efficient NAS (ENAS) \cite{pham2018efficient}, 
    assisted REA (AREA) \cite{https://doi.org/10.48550/arxiv.2006.04647}, 
    zero-cost NAS \cite{https://doi.org/10.48550/arxiv.2101.08134}, 
    TE-NAS \cite{https://doi.org/10.48550/arxiv.2102.11535}, gradient kernel-based 
    NAS (KNAS) \cite{xu2021knas}, RS with NASWOT \cite{https://doi.org/10.48550/arxiv.2006.04647}, 
    RS with noise immunity (NI) \cite{10092788}, and RS with LogSynFlow \cite{Cavagnero_2023}. 

    The three search spaces, NATS-bench-SSS, NATS-bench-TSS 
    \cite{Dong_2021} and NAS-Bench-101 \cite{ying2019nasbench101}, are used 
    here to evalute the performance of the NAS algorithms. The architectures 
    of NATS-bench-TSS is same as NAS-Bench-201 \cite{dong2020nasbench201}. 
    The architectures in NATS-bench-SSS and NATS-bench-TSS are trained by the three datasets: 
    CIFAR10, CIFAR100, ImageNet-16-120, while NAS-Bench-101 is trained by one: 
    CIFAR10. 

    \subsection{Environment and Parameter Settings}
    The experiment is conducted on a PC with Intel Core i7-11700K (3.60 GHz, 16-MB Cache, and 16 cores), 
    a single NVIDIA RTX3070 Ti GPU with 8 GB memory, driver version 520.61.05, CUDA version 11.8, and 
    65 GB available memory running on Ubuntu 20.04.1 with linux kernel 5.15.0-73-generic. All the program 
    is written in Python 3.7.16 with PyTorch 1.7.1+cu110 package. The simulations of all 
    nonweight sharing NASs are carried out for 500 runs on all the search spaces except AREA which is 50 runs. 
    For weight sharing NAS algorithms, each is carried out for 3 runs. And for training-free 
    algorithms, each is carried out for 100 runs and are given an evaluation 
    budget of 1000 for all the search space. The parameter settings of \palg are as follows: 
    initial temperature, ending temperature, rate of decrease of temperature, the number of iteration, 
    the constant number adjusting acceptance probability, the number of generated neighbours are set 
    to 1, 0.0008, 0.745, 4, 0.25, 10, so that the maximum number of searching candidates is 1000. 

    \subsection{NATS-bench-SSS}
    In \xtab{table:overall_sss}, the proposed NAS algorithm and ranking function is 
    compared with the other five nonweight sharing algorithm and NI, NASWOT, LogSynFlow. 
    For fairness, the searching candidates for the training-free score functions, i.e., 
    NI, NASWOT, LogSynFlow, Rank, are identical. The performance of ranking function 
    is similar to NI and lower than LogSynFlow. However, the proposed NAS algorithm 
    imporved the results significantly and outperforms almost all the compared algorithms. 

    %\begin{table*}[htb]
    %    %\normalsize
    %    %\centering
    %    \newcommand{\z}{\phantom{0}}
    %    \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NATS-Bench-SSS.}}
    %      \vspace{-\baselineskip}
    %    \begin{tabular}{@{}lccccccc@{}}\toprule
    %    Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
    %    & & validation & test & validation & test & validation & test \\ \midrule
    %    &&&&\textbf{Non-weight sharing}&&&\\
    %    AREA      & $12,000$ & $84.62 \pm \z{0.41}$                & $93.16 \pm 0.16$               & $59.24 \pm \z{1.11}$                 & $69.56 \pm \z{0.96}$                & $37.58 \pm 1.09$                 & $45.30 \pm \z{0.91}$                \\
    %    REA       & $12,000$ & $90.26 \pm \z{0.22}$                & $93.17 \pm 0.21$               & $69.48 \pm \z{0.76}$                 & $69.49 \pm \z{0.94}$                & $44.84 \pm 0.72$                 & $45.47 \pm \z{0.91}$                \\
    %    RS        & $12,000$ & $90.08 \pm \z{0.24}$                & $93.01 \pm 0.29$               & $69.14 \pm \z{0.94}$                 & $69.17 \pm \z{1.00}$                & $44.66 \pm 1.02$                 & $44.83 \pm \z{1.05}$                \\
    %    RL        & $12,000$ & $90.17 \pm \z{0.23}$                & $93.08 \pm 0.21$               & $69.23 \pm \z{0.87}$                 & $69.29 \pm \z{1.08}$                & $44.68 \pm 0.91$                 & $45.05 \pm \z{0.93}$                \\
    %    BOHB      & $12,000$ & $90.05 \pm \z{0.30}$                & $93.03 \pm 0.20$               & $69.04 \pm \z{0.76}$                 & $69.16 \pm \z{0.90}$                & $44.71 \pm 0.78$                 & $44.91 \pm \z{1.05}$                \\ \midrule
    %    %&& Search (s) & test & Search(s) & test & Search(s) & test \\ \midrule
    %    &&&&\textbf{Training-free}&&&\\
    %    NI (N=1,000)         & $X$ & $89.56\pm \z{0.147}$ & $92.55\pm 0.187$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    NASWOT (N=1,000)     & $X$ & $89.25\pm \z{0.412}$ & $92.21\pm 0.298$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    LogSynFlow (N=1,000) & $X$ & $89.61\pm \z{0.101}$ & $92.60\pm 0.188$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    rk (N=1000)          & $X$ & $89.59\pm \z{0.136}$ & $92.51\pm 0.171$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    GA-rk                & $457.54$ & $90.29\pm \z{0.149}$ & $93.27\pm0.193$ & $69.88\pm \z{0.497}$ & $70.06\pm \z{0.481}$ & $45.57\pm 0.425$ & $46.19\pm \z{0.846}$ \\ 
    %    SA-rk                & $682.36$ & $90.37\pm \z{0.204}$ & $93.29\pm0.182$ & $70.11\pm \z{0.457}$ & $70.32\pm \z{0.458}$ & $45.38\pm 0.499$ & $46.45\pm \z{0.687}$ \\ \bottomrule
    %    \end{tabular}
    %    \label{table:overall_sss}
    %      \vspace{-\baselineskip}
    %\end{table*}

    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NATS-Bench-SSS.}}
          \vspace{-\baselineskip}
        \begin{tabular}{@{}lccccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
                             &          & validation       & test             & validation       & test              & validation       & test \\ \midrule
                             &          &                  &                  & \textbf{Non-weight sharing} &        &                  &                  \\
        AREA                 & $12,000$ & $84.62 \pm 0.41$ & $93.16 \pm 0.16$ & $59.24 \pm 1.11$ & $69.56 \pm 0.96$  & $37.58 \pm 1.09$ & $45.30 \pm 0.91$ \\
        REA                  & $12,000$ & $90.37 \pm 0.20$ & $93.22 \pm 0.16$ & $70.23 \pm 0.50$ & $70.11 \pm 0.61$  & $45.30 \pm 0.69$ & $45.94 \pm 0.92$ \\
        RS                   & $12,000$ & $90.10 \pm 0.26$ & $93.03 \pm 0.25$ & $69.57 \pm 0.57$ & $69.72 \pm 0.61$  & $45.01 \pm 0.74$ & $45.42 \pm 0.86$ \\
        RL                   & $12,000$ & $90.25 \pm 0.23$ & $93.16 \pm 0.21$ & $69.84 \pm 0.59$ & $69.96 \pm 0.57$  & $45.06 \pm 0.77$ & $45.71 \pm 0.93$ \\
        BOHB                 & $12,000$ & $90.07 \pm 0.28$ & $93.01 \pm 0.24$ & $69.75 \pm 0.60$ & $69.90 \pm 0.60$  & $45.11 \pm 0.69$ & $45.56 \pm 0.81$ \\ \midrule
                             &          &                  &                  & \textbf{Training-free} &             &                  &                  \\
        NI (N=1,000)         & $293$    & $89.55 \pm 0.16$ & $92.56 \pm 0.21$ & $67.67 \pm 0.91$ & $67.61 \pm 1.06$  & $43.19 \pm 0.73$ & $43.22 \pm 0.53$ \\ 
        NASWOT (N=1,000)     & $208$    & $89.25 \pm 0.44$ & $92.21 \pm 0.31$ & $65.09 \pm 2.89$ & $64.61 \pm 3.16$  & $41.29 \pm 2.32$ & $41.35 \pm 2.31$ \\ 
        LogSynFlow (N=1,000) & $123$    & $89.60 \pm 0.11$ & $92.61 \pm 0.18$ & $68.23 \pm 0.49$ & $68.30 \pm 0.54$  & $43.58 \pm 0.58$ & $43.48 \pm 0.40$ \\ 
        rank (N=1000)        & $612$    & $89.59 \pm 0.14$ & $92.51 \pm 0.17$ & $67.56 \pm 0.75$ & $67.55 \pm 0.82$  & $43.36 \pm 0.62$ & $43.27 \pm 0.45$ \\ 
        \palg                & $712$    & $90.38 \pm 0.20$ & $93.30 \pm 0.19$ & $69.94 \pm 0.46$ & $70.24 \pm 0.42$  & $45.41 \pm 0.49$ & $46.39 \pm 0.69$ \\ \midrule
        \palg ver2           & $X$      & $X \pm X$        & $X \pm X$        & $X \pm X$        & $X \pm X$         & $X \pm X$        & $46.43 \pm 0.75$ \\ \midrule
        Optimal              &          & $90.71$          & $93.65$          & $70.92$          & $71.34$           & $46.73$          & $47.40$          \\ \bottomrule
        \end{tabular}
        \label{table:overall_sss}
          \vspace{-\baselineskip}
    \end{table*}

    \subsection{NATS-Bench-TSS}
    The result of NATS-Bench-TSS is shown in \xtab{table:overall_201}.
    %\begin{table*}[htb]
    %    %\normalsize
    %    %\centering
    %    \newcommand{\z}{\phantom{0}}
    %    \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-201.}}
    %      \vspace{-\baselineskip}
    %    \begin{tabular}{@{}lrcccccc@{}}\toprule
    %    Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
    %    & & validation & test & validation & test & validation & test \\ \midrule
    %    &&&&\textbf{Non-weight sharing}&&&\\
    %    AREA      & $12,000$ & $91.18 \pm \z{0.43}$                & $93.95 \pm \z{0.39}$               & $71.84 \pm \z{1.21}$                 & $71.92 \pm \z{1.29}$                & $45.04 \pm 1.03$                 & $45.40 \pm 1.14$                \\
    %    REA       & $12,000$ & $91.08 \pm \z{0.54}$                & $93.89 \pm \z{0.50}$               & $71.69 \pm \z{1.34}$                 & $71.83 \pm \z{1.33}$                & $44.96 \pm 1.41$                 & $45.30 \pm 1.51$                \\
    %    RS        & $12,000$ & $90.91 \pm \z{0.33}$                & $93.67 \pm \z{0.33}$               & $70.91 \pm \z{1.04}$                 & $70.99 \pm \z{0.99}$                & $44.52 \pm 0.99$                 & $44.56 \pm 1.25$                \\
    %    RL        & $12,000$ & $90.87 \pm \z{0.41}$                & $93.63 \pm \z{0.36}$               & $70.62 \pm \z{1.08}$                 & $70.77 \pm \z{1.05}$                & $44.20 \pm 1.22$                 & $44.23 \pm 1.37$                \\
    %    BOHB      & $12,000$ & $88.47 \pm \z{1.19}$                & $91.79 \pm \z{1.11}$               & $67.18 \pm \z{2.05}$                 & $67.50 \pm \z{2.05}$                & $38.94 \pm 3.58$                 & $39.00 \pm 3.73$                \\ \midrule
    %    &&&&\textbf{Weight sharing}&&&\\
    %    RSWS      & $4,154$  & $76.95 \pm 16.74$                   & $82.60 \pm 12.10$                  & $52.51 \pm 18.33$                    & $52.93 \pm 18.32$                   & $29.76 \pm 9.50$                 & $29.16 \pm 9.61$                \\
    %    DARTS-V1  & $5,475$  & $39.77 \pm \z{0.00}$                & $54.30 \pm \z{0.00}$               & $15.03 \pm \z{0.00}$                 & $15.61 \pm \z{0.00}$                & $16.43 \pm 0.00$                 & $16.32 \pm 0.00$                \\
    %    DARTS-V2  & $16,114$ & $39.77 \pm \z{0.00}$                & $54.30 \pm \z{0.00}$               & $15.03 \pm \z{0.00}$                 & $15.61 \pm \z{0.00}$                & $16.43 \pm 0.00$                 & $16.32 \pm 0.00$                \\
    %    GDAS      & $11,183$ & $90.05 \pm \z{0.23}$                & $93.46 \pm \z{0.13}$               & $71.02 \pm \z{0.31}$                 & $70.56 \pm \z{0.24}$                & $41.77 \pm 1.24$                 & $41.96 \pm 0.90$                \\
    %    SETN      & $16,787$ & $84.25 \pm \z{5.05}$                & $88.01 \pm \z{4.52}$               & $59.72 \pm \z{7.30}$                 & $59.91 \pm \z{7.51}$                & $33.93 \pm 3.85$                 & $33.48 \pm 4.22$                \\
    %    ENAS      & $7,061$  & $40.11 \pm \z{3.28}$                & $56.33 \pm \z{3.70}$               & $14.09 \pm \z{1.60}$                 & $14.77 \pm \z{1.45}$                & $16.20 \pm 0.48$                 & $15.93 \pm 0.67$                \\ \midrule
    %    &&&&\textbf{Training-free}&&&\\
    %    NI (N=1,000)         & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    NASWOT (N=1,000)     & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    LogSynFlow (N=1,000) & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    rk (N=1000)          & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
    %    GA-rk                & $747.25$ & $89.93\pm \z{0.196}$ & $93.41\pm0.083$ & $70.70\pm \z{0.417}$ & $70.76\pm \z{0.378}$ & $42.70\pm 1.315$ & $43.10\pm \z{1.428}$ \\ 
    %    SA-rk                & $X$ & $89.95\pm \z{0.194}$ & $93.37\pm0.114$ & $70.69\pm \z{0.391}$ & $70.75\pm \z{0.532}$ & $X\pm X$ & $43.10\pm \z{1.506}$ \\ \bottomrule
    %    \end{tabular}
    %    \label{table:overall_201}
    %      \vspace{-\baselineskip}
    %\end{table*}
    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-TSS.}}
          \vspace{-\baselineskip}
        \begin{tabular}{@{}lrcccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
                             &          & validation           & test             & validation           & test             & validation       & test \\ \midrule
                             &          &                      &                  & \textbf{Non-weight sharing} &           &                  &                  \\
        AREA                 & $12,000$ & $91.18 \pm \z{0.43}$ & $93.95 \pm 0.39$ & $71.84 \pm \z{1.21}$ & $71.92 \pm 1.29$ & $45.04 \pm 1.03$ & $45.40 \pm 1.14$ \\
        REA                  & $12,000$ & $91.25 \pm \z{0.31}$ & $94.02 \pm 0.31$ & $72.28 \pm \z{0.95}$ & $72.23 \pm 0.84$ & $45.71 \pm 0.77$ & $45.77 \pm 0.80$ \\
        RS                   & $12,000$ & $91.07 \pm \z{0.26}$ & $93.86 \pm 0.23$ & $71.46 \pm \z{0.97}$ & $71.55 \pm 0.97$ & $45.03 \pm 0.91$ & $45.28 \pm 0.97$ \\
        RL                   & $12,000$ & $91.12 \pm \z{0.25}$ & $93.90 \pm 0.26$ & $71.80 \pm \z{0.94}$ & $71.86 \pm 0.89$ & $45.37 \pm 0.74$ & $45.64 \pm 0.78$ \\
        BOHB                 & $12,000$ & $91.17 \pm \z{0.27}$ & $93.94 \pm 0.28$ & $72.04 \pm \z{0.93}$ & $72.00 \pm 0.86$ & $45.55 \pm 0.79$ & $45.70 \pm 0.86$ \\ \midrule
                             &          &                      &                  & \textbf{Weight sharing} &               &                  &                  \\
        RSWS                 & $4,154$  & $87.60 \pm \z{0.61}$ & $91.05 \pm 0.66$ & $68.27 \pm \z{0.72}$ & $68.26 \pm 0.96$ & $39.73 \pm 0.34$ & $40.69 \pm 0.36$ \\
        DARTS-V1             & $5,475$  & $49.27 \pm 13.44$    & $59.84 \pm 7.84$ & $61.08 \pm \z{4.37}$ & $61.26 \pm 4.43$ & $38.07 \pm 2.90$ & $37.88 \pm 2.91$ \\
        DARTS-V2             & $16,114$ & $58.78 \pm 13.44$    & $65.38 \pm 7.84$ & $59.48 \pm \z{5.13}$ & $60.49 \pm 4.95$ & $37.56 \pm 7.10$ & $36.79 \pm 7.59$ \\
        GDAS                 & $11,183$ & $89.68 \pm \z{0.72}$ & $93.23 \pm 0.13$ & $68.35 \pm \z{2.71}$ & $68.17 \pm 2.50$ & $39.55 \pm 0.00$ & $39.40 \pm 0.00$ \\
        SETN                 & $16,787$ & $90.00 \pm \z{0.97}$ & $92.72 \pm 4.52$ & $69.19 \pm \z{1.42}$ & $69.36 \pm 1.72$ & $39.77 \pm 0.33$ & $39.51 \pm 0.33$ \\
        ENAS                 & $7,061$  & $90.20 \pm \z{0.00}$ & $93.76 \pm 0.00$ & $70.21 \pm \z{0.71}$ & $70.67 \pm 0.62$ & $40.78 \pm 0.00$ & $41.44 \pm 0.00$ \\ \midrule
                             &          &                      &                  & \textbf{Training-free} &                &                  &                  \\
        NI (N=1,000)         & $X$      & $X \pm \z{X}$        & $93.21 \pm 0.63$ & $X \pm \z{X}$        & $70.40 \pm 0.84$ & $X \pm X$        & $43.34 \pm 2.43$ \\ 
        NASWOT (N=1,000)     & $X$      & $X \pm \z{X}$        & $92.80 \pm 1.07$ & $X \pm \z{X}$        & $69.66 \pm 1.23$ & $X \pm X$        & $43.84 \pm 2.93$ \\ 
        LogSynFlow (N=1,000) & $X$      & $X \pm \z{X}$        & $92.92 \pm 0.70$ & $X \pm \z{X}$        & $68.98 \pm 1.97$ & $X \pm X$        & $40.23 \pm 6.45$ \\ 
        rank (N=1000)        & $X$      & $X \pm \z{X}$        & $93.32 \pm 0.42$ & $X \pm \z{X}$        & $70.10 \pm 1.45$ & $X \pm X$        & $44.07 \pm 2.19$ \\ 
        \palg                & $X$      & $X \pm \z{X}$        & $\approx93\pm 2$ & $X \pm \z{X}$        & $70.30 \pm 1.26$ & $X \pm X$        & $\approx43\pm 4$ \\ \midrule
        \palg ver2           & $X$      & $X \pm X$            & $93.55 \pm 0.22$ & $X \pm X$            & $70.63 \pm 0.66$ & $X \pm X$        & $44.63 \pm 1.64$ \\ \midrule
        Optimal              &          & $91.61$              & $94.37$          & $73.49$              & $73.51$          & $46.73$          & $47.31$          \\ \bottomrule
        \end{tabular}
        \label{table:overall_201}
          \vspace{-\baselineskip}
    \end{table*}

      \subsection{NAS-Bench-101}
      The result of NAS-Bench-101 is shown in \xtab{table:overall_101}.
      \begin{table*}[t]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-101.}}
          \vspace{-\baselineskip}
        \resizebox{0.5 \textwidth}{!}{
            \begin{tabular}{@{}lrcc@{}}\toprule
            Method               & Search (s) & \multicolumn{2}{c}{CIFAR-10}        \\ \cmidrule(lr){3-4}
                                 &            & validation       & test             \\ \midrule
                                 &            & \textbf{Non-weight sharing} &       \\
            AREA                 & $12,000$   & $93.67 \pm 0.48$ & $93.02 \pm 0.48$ \\
            REA                  & $12,000$   & $93.63 \pm 0.50$ & $93.02 \pm 0.52$ \\ \midrule
                                 &            &            \textbf{Weight sharing}& \\
            DARTS-V1             & $20,483$   & $83.50 \pm 0.11$ & $83.74 \pm 0.03$ \\
            ENAS                 & $22,325$   & $93.83 \pm 0.28$ & $93.34 \pm 0.26$ \\ \midrule
                                 &            & \textbf{Training-free} &            \\
            Zero-Cost NASWOT     & $18,940$   & $91.72 \pm 1.80$ & $91.29 \pm 1.82$ \\
            NI (N=1,000)         & $280$      & $92.89 \pm 2.10$ & $92.55 \pm 1.33$ \\ 
            NASWOT (N=1,000)     & $162$      & $92.27 \pm 6.60$ & $92.10 \pm 4.44$ \\ 
            LogSynFlow (N=1,000) & $105$      & $91.81 \pm 1.52$ & $91.21 \pm 1.71$ \\ 
            rank (N=1000)        & $516$      & $92.44 \pm 1.42$ & $91.94 \pm 2.01$ \\ 
            \palg                & $374$      & $92.89 \pm 3.24$ & $92.77 \pm 1.01$ \\ \bottomrule
            \palg ver2           & $374$      & $93.16 \pm 0.97$ & $92.77 \pm 1.03$ \\ \bottomrule
            Optimal              &            & $X$              & $94.32$          \\ \bottomrule
            \end{tabular}
        }
        \label{table:overall_101}
          \vspace{-\baselineskip}
      \end{table*}

    \section{Conclusion}
    \label{sec:conclusion}
    In this paper, a novel concept of "complementary" is proposed and used as a basis to choose 
    traning-free score functions. The new ranking algorithm is then applied on simulated annealing 
    algorithm. The simulation result shows that the proposed algorithm, namely SA-rk, not only outperforms the most 
    state-of-art traning-free score functions, but also some of the nontraning-free methods. 
    The proposed algorithm also shows the robustness, which performs well between different search 
    spaces vary with neural network structures and size. 
    The limitation of SA-rk is the lack of versatility, since the score functions are mainly chosen 
    for common image recognition. That means SA-rk may not be applied on the search of architectures 
    designed for other usages, i.e, speech recognition, NLP. However, the problem is able to be solved 
    by choosing the corresponding complementary score functions. Some further questions are that how many 
    score functions are required for precisely estimating the performance of architectures and what are they? 
    
    \bibliographystyle{IEEEtran}
    \bibliography{main}
\end{document}

