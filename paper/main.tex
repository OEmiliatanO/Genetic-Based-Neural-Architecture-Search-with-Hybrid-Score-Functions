%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass[sigconf]{acmart}
\let\Bbbk\relax %% fix bug
\usepackage[utf8]{inputenc}

% =======================
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsopn}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{textcomp}

\usepackage{boxedminipage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{url}
\usepackage{times}
\usepackage{version}
% \usepackage[pdftex]{graphicx}
\usepackage{epsfig}
\usepackage{epsf}
%\usepackage{graphics}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\PassOptionsToPackage{bookmarks={false}}{hyperref}
%%%%%%%%%%%%
\usepackage{comment}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{dblfloatfix}
% ==========================
\input lstset.tex
\input macros.tex

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\algrenewcommand\alglinenumber[1]{\footnotesize #1}

\algnewcommand\algorithmicassume{\textbf{Assumption:}}
\algnewcommand\Assume{\item[\algorithmicassume]}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\Input{\item[\algorithmicinput]}

\begin{document}

\title{Rank-based Training-Free NAS Algorithm}

\author{Hsieh Cheng-Han}
\email{emiliaistruelove@gmail.com}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\author{Chun-Wei Tsai}
\email{cwtsai@mail.cse.nsysu.edu.tw}
\affiliation{%
  \institution{Department of Computer Science and Engineering, National Sun Yat-sen University}
  \streetaddress{70 Lienhai Rd}
  \city{Kaohsiung}
  \country{Taiwan}
}

\begin{abstract}

    % dont we
    % To address
    % To evaluate
    Although the training-free neural architecture search (NAS) are typically 
    faster than training-based NAS methods, however, the correlation between 
    the measurement and the final accuracy of an architecture is not well 
    enough in most cases. 
    To address this problem, we propose a rank-based training-free NAS 
    algorithm, which combines three complement training-free score functions 
    to evaluate an architecture by ranking. 
    More precisely, noise immunity (NI) and the correlation between binary 
    activation paterns, named as NASWOT, are used as measurement of image recognition 
    ability, and, SynFlow are used as measurement of trainability. 
    With that, a modified version of simulated annealing (SA) algorithm 
    can applies on and obtains a better performance on searching high accuracy 
    architectures. 
    To evaluate the performance of the proposed algorithm, this paper compared 
    it with several NAS algorithms, including weight-sharing methods, 
    non-weight-sharing methods, and several state-of-the-art training-free score 
    functions. The final result shows that in relatively larger search space, 
    like NATS-Bench SSS, the proposed algorithm outperforms most of NAS methods. 

\end{abstract}
\maketitle

\section{Introduction}
\label{sec:introduction}

% introduction
    Neural architecture search (NAS) has recently drawn a big amount of 
    attention, since the ability to automatically design a "good" neural 
    architecture. By leveraging machine learning algorithms \cite{https://doi.org/10.48550/arxiv.1611.01578}, 
    NAS algorithms can explore a search space, which is comprised of numerous 
    potential architectures, to find out a good architectures that outperform 
    those designed by human experts. Recently, the use of NAS is 
    widespread, from object detection \cite{https://doi.org/10.48550/arxiv.2111.13336}, 
    image recognition \cite{https://doi.org/10.48550/arxiv.2006.04647} 
    and speech recognition \cite{https://doi.org/10.48550/arxiv.2011.05649} 
    \cite{mehrotra2021nasbenchasr} to natural language 
    processing. \cite{jiang-etal-2019-improved} 
    \cite{https://doi.org/10.48550/arxiv.2006.07116} 
    \cite{https://doi.org/10.48550/arxiv.2005.14187} 

    % reduce 
    Despite the promising results of NAS, there are still many challenges 
    to conquer. One major problem is the extremely high computational 
    cost to search for an optimal architecture, which can make NAS impractical 
    for real-world applications, particularly on resource-constrained 
    platforms like embedded system. The reason why NAS is costly is that 
    during the searching, a candidate architecture must be trained to 
    evaluate how good of this architecture. 

    To overcome this challenge, recent works developed and proposed lots 
    of method which is so called training-free NAS. 
    % NASWOT
    For example, Mellor et al. \cite{https://doi.org/10.48550/arxiv.2006.04647} proposed the 
    measurement of the correlation between the binary activation paterns, 
    induced by the untrained network at two inputs, named as neural architecture 
    search without training (NASWOT). 
    % synflow
    On the other hand, Lee et al. \cite{lee2019snip} proposed purning parameters based on a saliency matric, 
    which then extended further by Wang et al. \cite{wang2020picking} and Tanaka et al. \cite{tanaka2020pruning}. 
    In \cite{tanaka2020pruning}, Tanaka et al. proposed Iterative Synaptic Flow Pruning 
    (SynFlow) algorithm which intends to deal with the layer collapse problem when purning a network. 
    The score function used in the algorithm is so-called \it{synaptic saliency} \rm{score}. 
    Later, Abdelfattah et al. \cite{abdelfattah2021zerocost} extended SynFlow to score 
    a network architecture by summing synaptic saliency score over all parameters in the model. 
    Cavagnero et al. \cite{Cavagnero_2023} found that SynFlow is likely to suffer from gradient explosion, then 
    proposed the LogSynFlow score function which prevents the problem. 
    % NTK
    For another example, Chen et al. \cite{https://doi.org/10.48550/arxiv.2102.11535} proposed to compute the condition 
    number of neural tangent kernel (NTK) \cite{https://doi.org/10.48550/arxiv.2203.09137} 
    \cite{https://doi.org/10.48550/arxiv.2109.00817}, which is used as the score to estimate the trainability of an 
    architecture. 

% what is unknown?
    % table放第一頁 不然就不要放
    % 做成圖表放第一頁
    However, \xtab{table:corr_score} shows most of score functions suffer from low correlation between 
    score values and the final accuracy of architectures, leading to a predicament 
    that no matter how good the search method is used, we can hardly find 
    an optimal architecture. 
    The major problem causes the low correlation is that a single score 
    function can only evaluate one perspect/characteristic of an architecture. 
    \begin{table}[tb]
        \caption{\textsc{The Kendall correlation between training-free score and test accuracy, evaluated on the three datasets of NATS-Bench \cite{Dong_2021}. 
        Each metric has been computed three times with different initialisations and the average is taken as final score.}}
        \resizebox{0.48\textwidth}{!}{
            \begin{tabular}{cccc}\toprule
                Training-free score function & CIFAR-10 & CIFAR-100 & ImageNet-16-120 \\ 
                \midrule
                \bf{NTK}                     & -0.33    & -0.30     & -0.39 \\
                \bf{Snip}                    & 0.45     & 0.47      & 0.41  \\
                \bf{Fisher}                  & 0.39     & 0.40      & 0.36  \\
                \bf{Grasp}                   & 0.28     & 0.35      & 0.35  \\
                \bf{NASWOT}                  & 0.61     & 0.62      & 0.60  \\
                \bf{SynFlow}                 & 0.57     & 0.56      & 0.56  \\
                \bf{LogSynFlow}              & 0.61     & 0.60      & 0.59  \\
                \bf{Rank (ours)}             & X        & X         & X     \\ \bottomrule
                \label{table:corr_score}
            \vspace{-\baselineskip}
            \end{tabular}
        }
    \end{table}
    \begin{figure*}[b]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/naswot.pdf}}
        \caption{A simple example to illustrate the procedure of NASWOT.}
        \label{fig:naswot}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}
% method
    To address the problem, we propose cooperating three complement score 
    functions by ranking, which allows every score functions evaluate an 
    architecture without losing fairness. 
    The first score function is noise immunity (NI) \cite{10092788}, as 
    an indicator of the ability to distinguish two images. 
    The second one is NASWOT, as score to estimate the ability of expression 
    of an architecture and also a complement of NI. 
    The last one is SNIP used as the measurement of trainability and 
    also as a complement to NI and NASWOT. 
    With these three complement score functions, an evaluation of an 
    architecture is not simply from a single aspect but three different 
    aspects, which is like estimate the weight of an object by not only 
    the length but its height and width. 
    %compares the different search algorithms for NAS, e.g., random search, genetic 
    %algorithm (GA) \cite{mitchell1998introduction}, simulated annealing algorithm (SA)
    %\cite{kirkpatrick1983optimization}. 

    % conclusion
    %The major contribution of this paper can be summarized as follow:
    %\begin{itemize}
    %    \item Cooperating three score functions to obtain better ability for searching top accuracy neural architectures. 
    %    \item Develope simulated annealing algorithms with rank-based score. 
    %\end{itemize}
    % expect what

    The remainder of this paper is organized as follows: \xsec{sec:related_work} provides the 
    detail about the three complement score functions. \xsec{sec:proposed} gives a detailed 
    description about the proposed method. \xsec{sec:results} begins with parameters setting and 
    provide the simulation results following is the experiment results in different search space. 
    The conclusion and further prospect are gived in \xsec{sec:conclusion}.

    \section{Related Works}
    \label{sec:related_work}
    \subsection{Training-free Score Functions}
    % naswot 對圖描述多一點
    In \cite{https://doi.org/10.48550/arxiv.2006.04647}, Mellor et al. proposed a 
    score function without the requirement for training which is named neural 
    architecture search without training (NASWOT). \xfig{fig:naswot} 
	gives a simple example to illustrate the procedure of NASWOT score function. 
    Consider a mini-batch of data $X=\{x_i\}^N_{i=1}$ passing through a neural network 
    architecture. The activated ReLU units in every layer of the architecture 
    form a binary code $c_i$ that define the linear region.
    The correlation between binary codes for the whole mini-batch can be examined 
    by computing the kernel matrix 
    \begin{equation}
        K_H=\begin{pmatrix}N_A-d_H(c_1,c_1)&\cdots&N_A-d_H(c_1,c_N)\\\vdots&\ddots&\vdots\\N_A-d_H(c_N,c_1)&\cdots&N_A-d_H(c_N,c_N)\end{pmatrix},
    \end{equation}
    where $N_A$ is the number of ReLU units and $d_H(c_i,c_j)$ is the hamming 
    distance between the binary code $c_i$ and $c_j$. 
    With the kernel matrix, the score of an architecture can be evaluated as 
    follow: 
    \begin{equation}
        s=\log\lvert K_H\rvert,
    \end{equation}
    The rationale behind is that, the more different between the binary codes 
    the better the architecture learns. And the determinant of the kernel matrix 
    measures how "different" they are by calculating the volume formed by the 
    row vector of $K_H$. 
    
    %to see how dissimilar the linear region activated by 
    %the ReLU units between two inputs. An architecture shall learn better 
    %when inputs are well separated.
	
    % ni 對圖描述多一點
    Wu et al. \cite{10092788} found, in some case, an architecture with high 
    NASWOT score may classify the same kind of input data into different classes. 
    To fix this defect, Wu et al. proposed using noise immunity (NI) to 
    evaluate an architecture. \xfig{fig:ni} gives an example to illustrate how 
    noise immunity evaluate an architecture. The score function picks a 
    mini-batch of data, denoted $X$, and then applies Gaussion noise on it. 
    The process can be defined by $X'=X+z$ where $z$ is the Gaussion noise. 
    By passing $X$ and $X'$ through the untrained architecture, then computing 
    the difference of square of each feature maps captured at all pooling layers, 
    denoted $\eta$. More specifically, first, calculate the matrix $\kappa$ defined by 
    \begin{equation}
        \label{equ:ni_kappa}
        \kappa=\begin{pmatrix}\frac{(\tau_{1,1}-\tau'_{1,1})^2}{\lvert \tau_{1,1}\rvert}&\cdots&\frac{(\tau_{1,N}-\tau'_{1,N})^2}{\lvert \tau_{1,N}\rvert}\\\vdots&\ddots&\vdots\\\frac{(\tau_{L,1}-\tau'_{L,1})^2}{\lvert \tau_{L,1}\rvert}&\cdots&\frac{(\tau_{L,N}-\tau'_{L,N})^2}{\lvert \tau_{L,N}\rvert}\end{pmatrix},
    \end{equation}
    where $L$ is the total number of pooling layers; $N$ the size of mini-batch, 
    and $\tau_{i,j}$ and $\tau'_{i,j}$ are the feature maps which $X$ passing and 
    the one perturbed by $X'$ at the $i$-th pooling layer and $j$-th input 
    data of the mini-batch, respectively. 
    Then $\eta$ can be calculated by 
    \begin{equation}
        \label{equ:ni_eta}
        \eta=\ln(\epsilon+e_L\kappa e_N),
    \end{equation}
    where $\epsilon$ is a small positive number, and $e_L$ and $e_N$ are a row 
    vector of 1's of size $L$ and a column vector of 1's of size $N$, respectively. 
    According to the fact that the input data are the same kind, the smaller $\eta$ 
    is, the better the noise immunity of the architecture is.

    \begin{figure*}[tb]
        \vspace{-\baselineskip}
        \resizebox{1.0\textwidth}{!}{\includegraphics{asset/ni.pdf}}
        \caption{A simple example to illustrate the procedure of noise immunity.}
        \label{fig:ni}
        \centering
        \vspace{-\baselineskip}
    \end{figure*}

    % synflow
    % 再調
    Besides from these two image-related aspects, there is another aspect to evaluate  
    a network. 
    \textit{The Lottery Ticket Hypothesis} \cite{frankle2019lottery}, reveals that a 
    network may have a "core" which decides the final accuracy of the network. 
    How to pruning a network correctly is therefore tempting. Lee et al. 
    \cite{lee2019snip} proposed to use connection sensitivity as a criterion to prune 
    the network which can be briefly viewed as 
    \begin{equation}
        \label{equ:snip_connection_sensitivity}
        s_j=\lvert \frac{\partial \mathcal L}{\partial w_j}w_j\rvert,
    \end{equation}
    where $\textbf{w}$ is the weight of the netwrok, $w_j$ the $j$-th element of 
    $\textbf{w}$, and $\mathcal L$ is the empirical risk. The meaning behind is to approximate the contribution 
    to the change of the loss function from a specific connection. By pruning the 
    connections which has relatively small contribution, the expensive prune, retrain 
    cycles, can be prevented. 
    Later, Wang et al. \cite{wang2020picking} noticed that SNIP with a high pruning ratio 
    tends to cause \textit{layer-collapse}, which prunes all parameters in a single weight 
    layer. Therefore, they propose Gradient Signal Preservation (GraSP) algorithm, which 
    aims to preserve the gradient flow at initialization by approximating the change in 
    gradient norm defined as 
    \begin{equation}
        \label{equ:grap}
        S_p(\theta)=-(\textbf{H}\frac{\partial \mathcal L}{\partial \theta})\odot\theta,
    \end{equation}
    where $\textbf{H}$ is the Hessian matrix, $\theta$ the parameters, and $\odot$ is Hadamard product. 
    In \cite{tanaka2020pruning}, to solve the problem that the existing pruning algorithms, e.g., 
    Magnitude, SNIP, GraSP, using global-masking usually encounter layer-collapse which will make 
    the pruned network untrainable. 
    Tanaka et al. generalized the synaptic saliency scores as 
    \begin{equation}
        \label{equ:synflow}
        S_p(\theta)=\frac{\partial \mathcal R}{\partial \theta}\odot\theta,
    \end{equation}
    where $\mathcal R$ is a scalar loss function, and proposed Iterative Synaptic Flow Pruning (SynFlow) 
    algorithm which is an extension of magnitude pruning algorithm and avoids layer-collapse. 
    Abdelfattah et al. \cite{abdelfattah2021zerocost} extended the work, proposed to score a 
    network by summing the synaptic saliency score over all parameters in the model, which is 
    defined as 
    \begin{equation}
        \label{equ:zero_cost}
        S_n=\sum^N_i S_p(\theta)_i.
    \end{equation}
    The rationale behind is to calculate all the contribution to the loss function of parameters. 
    The higher the score of an architecture, the more trainable this architecture is. 
    Finally, Cavagnero et al. \cite{Cavagnero_2023} imporved SynFlow, which is likely to fall into 
    gradient explosion problem, and proposed LogSynFlow which simply scaling down the gradient. 
    The formula is defined by 
    \begin{equation}
        \label{equ:logsynflow}
        S(\theta)=\theta\cdot\log(\frac{\partial \mathcal L}{\partial \theta}+1)
    \end{equation}

    \subsection{Search Algorithms}
    First assume that input data $D$ are separated into two subset, $D_r$ and $D_t$. Further more, 
    assume $\mathcal F_A$ is the accuracy function, $\mathcal F_S$ the score function, $\mathcal A_s$ the 
    search algorithm of NAS, and $\mathcal A_L$ the learning algorithm. Then, NAS problem can be 
    described as an optimization problem as follows: 
    \begin{equation}
        \label{equ:nas}
        \mathbb N^*=\max_{\mathbb N^b\in\mathbb N}\mathcal F_A(\mathcal A_L(\mathbb N^b, D_r), D_t)
    \end{equation}
    where $\mathbb N$ is a set of neural architectures, namely, the search space of NAS. 
    For non-training-free NAS 
    \begin{equation}
        \label{equ:non-training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_A(\mathcal A_L(\mathbb N^s, D_r), D_t), \mathbb N)
    \end{equation}
    and for training-free NAS 
    \begin{equation}
        \label{equ:training_free_nas}
        \mathbb N^b=\mathcal A_S(\mathcal F_S(\mathbb N^s, D_r), \mathbb N)
    \end{equation}
    It can be seen that training-free NAS is comprised of two part, search algorithm $\mathcal A_S$ and score 
    function $\mathcal F_S$. 
    There are several search algorithms applied on NAS, including random search, reinforcement learning, and
    metaheuristic algorithm. 
    
    In \cite{https://doi.org/10.48550/arxiv.2006.04647} \cite{Lopes_2021}, random search is applied on. 
    The advantage of random search is simple, easy to implement, and the result can be token as a 
    baseline compared to more comprehensive search algorithm. Generally speaking, when applying random 
    search on a samll search space, e.g., nasbench201 \cite{dong2020nasbench201}, the performance is 
    similar to other search algorithms. But when it comes to a relatively larger search space, e.g., nasbench101 
    \cite{ying2019nasbench101} and natsbenchsss \cite{Dong_2021}, random search can no longer standout 
    in other search algorithms. 

    In \cite{zoph2017neural}, reinforcement learning is used to find the maximum accuracy of an architecture 
    generated by a network architecture controller. The actions $a_{1:T}$ of the reinforcement learning is 
    equivalent to updating the parameters $\theta_c$ for the controller. More specifically, the goal is to maximize 
    its expected reward, defined by 
    \begin{equation}
        \label{equ:reinforcement_rw}
        J(\theta_c)=E_{P(a_{1:T};\theta_c)}[R]
    \end{equation}
    and the gradient of $J(\theta_c)$ is defined by
    \begin{equation}
        \label{equ:reinforcement_grad}
        \nabla_{\theta_c} J(\theta_c)=\sum^T_t=1 E_{P(a_{1:T};\theta_c)}[\nabla_{\theta_c}\log P(a_t\mid a_{(t-1):1};\theta_c)R]
    \end{equation}

    As for an example of metaheuristic algorithm, in \cite{10.1145/3491396.3506510}, Wu et al. leveraged 
    genetic algorithm (GA) as search strategy. 
    By encoding the network architecture, the architecture can be viewed as gene. Therefore, GA can be 
    easily applied on. 
    Later in \cite{10092788}, Wu et al. used search economic (SE) \cite{7379579} as search strategy. The 
    basic idea of SEs is to first divide the search space into a set of subspaces and investiagte those 
    subspaces based on the expected value of each subspace. The expected value is comprised of 
    \begin{itemize}
        \item The number of times the subspace has been investiagted. 
        \item The average objective value of the new candidate solutions in the subspace at current iteration. 
        \item The best solution so far in this subspace. 
    \end{itemize}
    Based on these design, SE can avoid fall into local optimum, while search for high expected value instead. 
    
    \section{Methods}
    \label{sec:proposed}

    \subsection{Motivation}
    The motivation origins from these three questions: 

    \textit{1. Can we combine multiple score functions to improve the correlation between score values and the final accuracy of architectures?}
    
    The answer is yes. It's konwn that if the chosen score functions can complement with 
    each others, the new correlation can eventually beyond the original ones. \cite{10.1145/3491396.3506510} 
    shows that possibility. When an architecture gain a high NASWOT score, it may classify 
    images, which are in the same class originally, into different classes. NI here comes 
    to rescue. By measuring the noise immunity of an architecture, the miss-classifying 
    can be solved. Thus, if mix NASWOT with NI, the correlation increases. An example 
    is shown in \xfig{fig:ninaswot}.
    \begin{figure*}[htb]
        \vspace{-\baselineskip}
        \begin{center}
            \begin{tabular}{ccc}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/naswot-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ni-acc.pdf}}}
                \subfigure[]{\resizebox{0.33\textwidth}{!}{\includegraphics{asset/ninaswot-acc.pdf}}}
            \end{tabular}
            \caption{(a) NASWOT score for 1,000 randomly chosen architectures from NAS-Bench-201 in the CIFAR-10 dataset 
            (b) NI score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset. 
            (c) NI score + NASWOT score for 1,000 identical architectures from NAS-Bench-201 in the CIFAR-10 dataset.}
            \label{fig:ninaswot}
        \end{center}
        \vspace{-\baselineskip}
    \end{figure*}

    \textit{2. How to combine multiple different kinds of score functions together and prevent one from overwhelming the others numerically?}
    
    One possible way is normalization, but the necessary information is known only 
    after evaluating the whole search space, e.g., range, mean or standard deviation, which is practically 
    impossible. In order to achieve the goal, the proposed score function, named rank-based NAS, leverage 
    multiple score functions by ranking, which provides a solution to have equal contribution from each 
    score function. 

    \textit{3. How to choose the score functions?}

    As mentioned, the key is to choose the complement of a score function. For example, 
    NASWOT and NI. And for NASWOT and NI, which are both description of the ability of 
    the graphic recognition of an architecture. The complement of them can be a measurement 
    of trainability, e.g., SynFlow, NTK, which means whether this architecture is easy to 
    train or not. In this paper, NI, NASWOT, and LogSynFlow are used in the ranking algorithm. 

    \subsection{The Ranking Algorithm}
    \begin{algorithm}[h]
        \caption{The Ranking Algorithm}\label{alg:rank-based}
        \begin{algorithmic}[1]
            \Input{a subspace, ${\mathbb N}^{s}$, from search space, ${\mathbb N}^{*}$}
            \For{$\text{each }{\mathbb N}^i\text{ in }{\mathbb N}^s$}
                \For{$\text{each }j\in\{1,\ldots,M\}$}
                    \State $s_{{\mathcal F}_{j}}({\mathbb N}^i)={\mathcal F}_{j}({\mathbb N}^i)$
                \EndFor
            \EndFor
            \For{$\text{each }j\in\{1,\ldots,M\}$}
                \State $r_{{\mathcal F}_{j}}({\mathbb N}^i)$ = index of ${\mathbb N}^i$ in list $[s_{{\mathcal F}_{j}}({\mathbb N}^1),\ldots,s_{{\mathcal F}_{j}}({\mathbb N}^{\lvert {\mathbb N}^s\rvert})]$ \par sorted by descending order
            \EndFor
            \State $r({\mathbb N}^i)=\sum^M_{j=0} r_{{\mathcal F}_{j}}({\mathbb N}^i)$
            %\State $i^*=\arg\min_{i}\{r({\mathbb N}^i)\}$
            %\State $\textbf{Return }{\mathbb N}^{i^*}$
            \State $\textbf{Return }r$
        \end{algorithmic}
    \end{algorithm}

    The proposed ranking algorithm is outlined in \xalg{alg:rank-based}. 
    The first step is to evaluate the network architectures in the subset, 
    denoted $\mathbb N^s$, of search space, denoted $\mathbb N^*$. The 
    evaluation of an architecture determined by $j$-th score function is 
    denoted $s_{\mathcal F_j}(\mathbb N^i)$. After evaluating the subset 
    of search space, the rank of each score function is calculated, denoted 
    $r_{\mathcal F}$. Then the final rank, $r$, calculated by summing up 
    the ranks. If a network architecture gains higher score in these score 
    functions, the final rank should be higher (smaller index) too. Therefore, 
    the highest (minimum) rank in $r$ shall be the best network architecture. 
    In summary, the rank-based score function makes the contribution of 
    each score function even, and therefore evaluate an architecture from 
    different aspects. In this paper, NI and NASWOT are used as a complement 
    set, which take the ability of images generalization and distinction. 
    Lastly, LogSynFlow complements NI and NASWOT which takes the trainability 
    into account. 

    \subsection{Simulated Annealing with Rank-Based Score function}

    \begin{algorithm}[h]
        \caption{The Simulated Annealing with Ranking Algorithm}\label{alg:SA}
        \begin{algorithmic}[1]
            \Input{search space $\mathbb N^*$, initial temperature $T$, ending temperature $\tau$, 
            a real number between 0 and 1, $\alpha$, the number of iteration, $I$}
            \State $s^*=s=\text{sample a random architecture from }\Bbb N^*$
            \While{$T>\tau$}
                \For{$\text{ each }i \in [1\ldots I]$}
                    \State $r=\text{Ranking}(\{s\}\cup U(s))$ \Comment{Ranking $s$ with the neighbourhood of $s$, denoted $U(s)$}
                    \State $s^*=\arg\min\{r(\Bbb N^i)\}$ \Comment{Update the expected best algorithm}
                    \State $\Delta E=r(s^*)-r(s)$
                    \State $x\sim \mathcal U(0,1)$ \Comment{Sample $x$ from a uniform distribution between 0 and 1}
                    \If{$x\geq e^{\frac{\Delta E}{T}}$} \Comment{Accept by probability}
                        \State $s\leftarrow s^*$
                    \EndIf
                \EndFor
                \State $T\leftarrow T\times \alpha$
            \EndWhile
            \State $\textbf{Return }s^*$ \Comment{Return the best architecture}
        \end{algorithmic}
    \end{algorithm}

    The modified version of the simulated annealing algorithm with ranking algorithm 
    is outlined in \xalg{alg:SA}. The first step is to sample a architecture, denoted 
    $s$, from search space. Then use $s$ as a "seed" to generate the neighbourhood of it, 
    applying ranking algorithm on the neighbourhood, $\{s\}\cup U(s)$. After ranking, 
    the expected best architecture can be determined and update, but the seed for next 
    iteration will be replaced by $s^*$ according to probability, $e^{\frac{\Delta E}{T}}$.
    After the $I$ iterations, the temperature scales down by $\alpha$. The algorithm 
    is done when $T\leq\tau$, and the best architecture will be retruned. 

    %\begin{figure*}[htb]
    %    \centering
    %    \hspace*{-\baselineskip}
    %    \vspace*{-\baselineskip}
    %    \includegraphics[]{asset/SA.pdf}
    %    \caption{A simple example to illustrate an iteration of SA with Rank-based score function.}
    %    \label{fig:SA}
    %\end{figure*}
    %\xfig{fig:SA} illustrates how SA with rank-based score function works in an iteration. 

    \section{experiment result}
    \label{sec:results}
    \subsection{Environment and Parameter Settings}
    The experiment is conducted on a PC with Intel Core i7-11700K (3.60 GHz, 16-MB Cache, and 16 cores), 
    a single NVIDIA RTX3070 Ti GPU with 8 GB memory, driver version 520.61.05, CUDA version 11.8, and 
    65 GB available memory running on Ubuntu 20.04.1 with linux kernel 5.15.0-73-generic. All the program 
    is written in Python 3.7.16 with PyTorch 1.7.1+cu110 package. 
    \subsection{NATS-bench-SSS}
    The result of NATS-bench-SSS is shown in \xtab{table:overall_sss}.
    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NATS-Bench-SSS.}}
          \vspace{-\baselineskip}
        \begin{tabular}{@{}lccccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
        & & validation & test & validation & test & validation & test \\ \midrule
        &&&&\textbf{Non-weight sharing}&&&\\
        AREA      & $12,000$ & $84.62 \pm \z{0.41}$                & $93.16 \pm 0.16$               & $59.24 \pm \z{1.11}$                 & $69.56 \pm \z{0.96}$                & $37.58 \pm 1.09$                 & $45.30 \pm \z{0.91}$                \\
        REA       & $12,000$ & $90.26 \pm \z{0.22}$                & $93.17 \pm 0.21$               & $69.48 \pm \z{0.76}$                 & $69.49 \pm \z{0.94}$                & $44.84 \pm 0.72$                 & $45.47 \pm \z{0.91}$                \\
        RS        & $12,000$ & $90.08 \pm \z{0.24}$                & $93.01 \pm 0.29$               & $69.14 \pm \z{0.94}$                 & $69.17 \pm \z{1.00}$                & $44.66 \pm 1.02$                 & $44.83 \pm \z{1.05}$                \\
        RL        & $12,000$ & $90.17 \pm \z{0.23}$                & $93.08 \pm 0.21$               & $69.23 \pm \z{0.87}$                 & $69.29 \pm \z{1.08}$                & $44.68 \pm 0.91$                 & $45.05 \pm \z{0.93}$                \\
        BOHB      & $12,000$ & $90.05 \pm \z{0.30}$                & $93.03 \pm 0.20$               & $69.04 \pm \z{0.76}$                 & $69.16 \pm \z{0.90}$                & $44.71 \pm 0.78$                 & $44.91 \pm \z{1.05}$                \\ \midrule
        %&& Search (s) & test & Search(s) & test & Search(s) & test \\ \midrule
        &&&&\textbf{Training-free}&&&\\
        NI (N=1,000)         & $X$ & $89.56\pm \z{0.147}$ & $92.55\pm 0.187$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        NASWOT (N=1,000)     & $X$ & $89.25\pm \z{0.412}$ & $92.21\pm 0.298$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        LogSynFlow (N=1,000) & $X$ & $89.61\pm \z{0.101}$ & $92.60\pm 0.188$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        rk (N=1000)          & $X$ & $89.59\pm \z{0.136}$ & $92.51\pm 0.171$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        GA-rk                & $457.54$ & $90.29\pm \z{0.149}$ & $93.27\pm0.193$ & $69.88\pm \z{0.497}$ & $70.06\pm \z{0.481}$ & $45.57\pm 0.425$ & $46.19\pm \z{0.846}$ \\ 
        SA-rk                & $682.36$ & $90.37\pm \z{0.204}$ & $93.29\pm0.182$ & $70.11\pm \z{0.457}$ & $70.32\pm \z{0.458}$ & $45.38\pm 0.499$ & $46.45\pm \z{0.687}$ \\ \bottomrule
        \end{tabular}
        \label{table:overall_sss}
          \vspace{-\baselineskip}
      \end{table*}

    \subsection{NAS-Bench-201}
    The result of NAS-Bench-201 is shown in \xtab{table:overall_201}.
    \begin{table*}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-201.}}
          \vspace{-\baselineskip}
        \begin{tabular}{@{}lrcccccc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{ImageNet-16-120} \\ \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
        & & validation & test & validation & test & validation & test \\ \midrule
        &&&&\textbf{Non-weight sharing}&&&\\
        AREA      & $12,000$ & $91.18 \pm \z{0.43}$                & $93.95 \pm \z{0.39}$               & $71.84 \pm \z{1.21}$                 & $71.92 \pm \z{1.29}$                & $45.04 \pm 1.03$                 & $45.40 \pm 1.14$                \\
        REA       & $12,000$ & $91.08 \pm \z{0.54}$                & $93.89 \pm \z{0.50}$               & $71.69 \pm \z{1.34}$                 & $71.83 \pm \z{1.33}$                & $44.96 \pm 1.41$                 & $45.30 \pm 1.51$                \\
        RS        & $12,000$ & $90.91 \pm \z{0.33}$                & $93.67 \pm \z{0.33}$               & $70.91 \pm \z{1.04}$                 & $70.99 \pm \z{0.99}$                & $44.52 \pm 0.99$                 & $44.56 \pm 1.25$                \\
        RL        & $12,000$ & $90.87 \pm \z{0.41}$                & $93.63 \pm \z{0.36}$               & $70.62 \pm \z{1.08}$                 & $70.77 \pm \z{1.05}$                & $44.20 \pm 1.22$                 & $44.23 \pm 1.37$                \\
        BOHB      & $12,000$ & $88.47 \pm \z{1.19}$                & $91.79 \pm \z{1.11}$               & $67.18 \pm \z{2.05}$                 & $67.50 \pm \z{2.05}$                & $38.94 \pm 3.58$                 & $39.00 \pm 3.73$                \\ \midrule
        &&&&\textbf{Weight sharing}&&&\\
        RSWS      & $4,154$  & $76.95 \pm 16.74$                   & $82.60 \pm 12.10$                  & $52.51 \pm 18.33$                    & $52.93 \pm 18.32$                   & $29.76 \pm 9.50$                 & $29.16 \pm 9.61$                \\
        DARTS-V1  & $5,475$  & $39.77 \pm \z{0.00}$                & $54.30 \pm \z{0.00}$               & $15.03 \pm \z{0.00}$                 & $15.61 \pm \z{0.00}$                & $16.43 \pm 0.00$                 & $16.32 \pm 0.00$                \\
        DARTS-V2  & $16,114$ & $39.77 \pm \z{0.00}$                & $54.30 \pm \z{0.00}$               & $15.03 \pm \z{0.00}$                 & $15.61 \pm \z{0.00}$                & $16.43 \pm 0.00$                 & $16.32 \pm 0.00$                \\
        GDAS      & $11,183$ & $90.05 \pm \z{0.23}$                & $93.46 \pm \z{0.13}$               & $71.02 \pm \z{0.31}$                 & $70.56 \pm \z{0.24}$                & $41.77 \pm 1.24$                 & $41.96 \pm 0.90$                \\
        SETN      & $16,787$ & $84.25 \pm \z{5.05}$                & $88.01 \pm \z{4.52}$               & $59.72 \pm \z{7.30}$                 & $59.91 \pm \z{7.51}$                & $33.93 \pm 3.85$                 & $33.48 \pm 4.22$                \\
        ENAS      & $7,061$  & $40.11 \pm \z{3.28}$                & $56.33 \pm \z{3.70}$               & $14.09 \pm \z{1.60}$                 & $14.77 \pm \z{1.45}$                & $16.20 \pm 0.48$                 & $15.93 \pm 0.67$                \\ \midrule
        &&&&\textbf{Training-free}&&&\\
        NI (N=1,000)         & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        NASWOT (N=1,000)     & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        LogSynFlow (N=1,000) & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        rk (N=1000)          & $X$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ & $X\pm \z{X}$ & $X\pm X$ & $X\pm \z{X}$ \\ 
        GA-rk                & $747.25$ & $89.93\pm \z{0.196}$ & $93.41\pm0.083$ & $70.70\pm \z{0.417}$ & $70.76\pm \z{0.378}$ & $42.70\pm 1.315$ & $43.10\pm \z{1.428}$ \\ 
        SA-rk                & $X$ & $89.95\pm \z{0.194}$ & $93.37\pm0.114$ & $70.69\pm \z{0.391}$ & $70.75\pm \z{0.532}$ & $X\pm X$ & $43.10\pm \z{1.506}$ \\ \bottomrule
        \end{tabular}
        \label{table:overall_201}
          \vspace{-\baselineskip}
      \end{table*}

      \subsection{NAS-Bench-101}
      The result of NAS-Bench-101 is shown in \xtab{table:overall_101}.
      \begin{table}[htb]
        %\normalsize
        %\centering
        \newcommand{\z}{\phantom{0}}
        \caption{\textsc{Comparison of rank-based NAS and all the other NAS algorithms in NAS-Bench-101.}}
          \vspace{-\baselineskip}
        \resizebox{0.5 \textwidth}{!}{
        \begin{tabular}{@{}lrcc@{}}\toprule
        Method & Search (s) & \multicolumn{2}{c}{CIFAR-10} \\ \cmidrule(lr){3-4}
               &            & validation & test \\ \midrule
               &            &            \textbf{Non-weight sharing}&\\
        AREA   & $12,000$   & $93.67 \pm \z{0.48}$  & $93.02 \pm \z{0.48}$\\
        REA    & $12,000$   & $93.63 \pm \z{0.50}$  & $93.02 \pm \z{0.52}$\\ \midrule
               &            &            \textbf{Weight sharing}&\\
        DARTS-V1 & $20,483$ & $83.50 \pm \z{0.11}$ & $83.74 \pm \z{0.03}$ \\
        ENAS     & $22,325$ & $93.83 \pm \z{0.28}$ & $93.34 \pm \z{0.26}$ \\ \midrule
                 &          &            \textbf{Training-free}&\\
        Zero-Cost NASWOT     & $18,940$  & $91.72 \pm \z{1.80}$ & $91.29 \pm \z{1.82}$ \\
        NI (N=1,000)         & $280$ & $93.15 \pm \z{1.48}$ & $92.72 \pm \z{1.06}$ \\ 
        NASWOT (N=1,000)     & $162$ & $92.40 \pm \z{4.07}$ & $91.97 \pm \z{4.20}$ \\ 
        LogSynFlow (N=1,000) & $105$ & $90.70 \pm \z{8.41}$ & $90.98 \pm \z{2.56}$ \\ 
        rank (N=1000)        & $516$ & $92.84 \pm \z{1.19}$ & $92.51 \pm \z{1.14}$ \\ 
        GA-rank              & $X$   & $X \pm X$ & $X\pm X$\\ 
        SA-rank              & $374$ & $93.01 \pm \z{1.19}$ & $92.37 \pm \z{1.63}$  \\ \bottomrule
        \end{tabular}
        }
        \label{table:overall_101}
          \vspace{-\baselineskip}
      \end{table}
    
    \section{Conclusion}
    \label{sec:conclusion}

    \bibliographystyle{IEEEtran}
    \bibliography{main}
\end{document}

